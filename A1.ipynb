{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak-hannou/compsci-4ml3/blob/main/F24_4ML3_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mug_cKdlFtiG"
      },
      "source": [
        "# Assignment 1 - Coding Section\n",
        "\n",
        "## Follow the codes, comments and the explanations. There are 3 problems in total which are worth 70 points. You are going to add your code in this notebook.\n",
        "\n",
        "### For this assignment, we will create a dataset consisting of black-and-white images of circles. The task is to estimate the radius and location of the ball based on the image using various Least Squares techniques.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ***Important Note:***\n",
        "\n",
        "## You need to write your report, including the results, plots, and analyses/discussion, in a PDF file. Submit this PDF file along with your theory solutions. Additionally, upload your Jupyter notebook (.ipynb) file alongside the PDF.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d532ZIBzKDkM"
      },
      "source": [
        "# Section One: Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e85Fn5yDoycd"
      },
      "source": [
        "## Importing necessary libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oWZ3u-ZK2l-F"
      },
      "outputs": [],
      "source": [
        "import numpy as np                # NumPy: Numerical computing with arrays and matrices\n",
        "import pandas as pd               # Pandas: Data manipulation and analysis with DataFrames\n",
        "import cv2                        # OpenCV: Computer vision, image, and video processing\n",
        "import random                     # Random: Generating random numbers and selections\n",
        "import matplotlib.pyplot as plt   # Matplotlib: Creating visualizations and plots\n",
        "from PIL import Image, ImageDraw  # Pillow: Image file opening, manipulation, and saving\n",
        "from sklearn.model_selection import train_test_split  # Scikit-learn: Splitting datasets for machine learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15p39AfEpKGi"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_SIkZcT_26gZ"
      },
      "outputs": [],
      "source": [
        "image_size = 30                       # Size of the generated images (30x30 pixels)\n",
        "min_radius = 3                        # Minimum circle radius in pixels\n",
        "max_radius = image_size // 2 - 2      # Maximum circle radius, ensuring circles fit within the image\n",
        "total_size = 2200                     # Total number of images to generate\n",
        "noise_level = 0.8                     # Noise level to add to the images\n",
        "\n",
        "csv_filename = 'generated_images_masks.csv'  # Filename for saving the generated data as a CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-GUqd-fO4m4"
      },
      "source": [
        "## Generating the Dataset\n",
        "\n",
        "This code generates a synthetic dataset of binary images with circular objects embedded within them.\n",
        "\n",
        "- **Image Size**: Each generated image is 30x30 pixels.\n",
        "- **Circle Properties**:\n",
        "  - **Radius**: Randomly chosen between 3 and 13 pixels (ensuring the circle fits within the image boundaries).\n",
        "  - **Position**: The center of the circle is randomly positioned within the image while ensuring the circle remains fully inside the image boundaries.\n",
        "- **Noise Level**: Random noise is added to each image, controlled by a `noise_level` parameter (default is 0.8).\n",
        "- **Total Images**: A total of 2,200 images are generated.\n",
        "- **Metadata**: Each image is accompanied by metadata, including the circle's radius and its center coordinates `(x, y)`.\n",
        "\n",
        "The dataset is saved as a CSV file named `generated_images_masks.csv`, where each row corresponds to a single image, with the first three columns representing the metadata (`radius`, `x`, `y`) and the remaining columns representing the flattened pixel values of the binary image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Dataset Dimensions\n",
        "\n",
        "- **Feature Dimensions**: The dataset includes 900 pixel values per image. These pixel values are flattened into a 1D array to create the feature set for each image.\n",
        "- **Target Variables**: The target variables in the dataset are the circle's radius (`radius`) and its center coordinates (`x`, `y`).\n",
        "- **Overall Dataset Dimensions**: The resulting dataset has 2,200 rows (one for each image) and 903 columns (3 columns for metadata: `radius`, `x`, `y`, and 900 columns for the pixel values).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "X6fYr-yr7aRa"
      },
      "outputs": [],
      "source": [
        "def generate_image_with_metadata(image_size, radius, x, y, noise_level=0.8):\n",
        "    \"\"\"\n",
        "    Generate a binary mask with a circle, add random noise, attach metadata (radius, x, y), and return the flattened mask.\n",
        "\n",
        "    noise_level: Controls the amount of random noise added (default is 0.8).\n",
        "    \"\"\"\n",
        "    # Create a black background image\n",
        "    img = Image.new('1', (image_size, image_size), color=0)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Draw the circle on the binary mask\n",
        "    draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=1)\n",
        "\n",
        "    # Add random noise to the image\n",
        "    noise = np.random.rand(image_size, image_size) > noise_level  # Generate random noise\n",
        "    noisy_mask = np.logical_and(img, noise).astype(np.float32)  # Apply noise to the mask\n",
        "\n",
        "    # Flatten the noisy mask into a 1D array\n",
        "    flattened_mask = noisy_mask.flatten()\n",
        "\n",
        "    # Store the metadata (radius, x, y)\n",
        "    metadata = [radius, x, y]\n",
        "\n",
        "    # Combine metadata with the binary mask\n",
        "    return np.array(metadata + flattened_mask.tolist())\n",
        "\n",
        "\n",
        "def generate_images_with_masks(total_size, image_size, min_radius, max_radius):\n",
        "    \"\"\"\n",
        "    Generate multiple images with masks by calling the image generation function.\n",
        "    \"\"\"\n",
        "    data = []  # List to store masks and metadata\n",
        "\n",
        "    for _ in range(total_size):\n",
        "        # Randomly choose the radius and center (x, y)\n",
        "        radius = random.randint(min_radius, max_radius)\n",
        "        x = random.randint(radius + 1, image_size - radius - 1)\n",
        "        y = random.randint(radius + 1, image_size - radius - 1)\n",
        "\n",
        "        # Generate a single image with mask and metadata\n",
        "        single_image_data = generate_image_with_metadata(image_size, radius, x, y, noise_level)\n",
        "        data.append(single_image_data)\n",
        "\n",
        "    return np.array(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEa8xXOH8dHu"
      },
      "outputs": [],
      "source": [
        "# Generate images with masks (returns a dataset including radius, x, y, and pixel values)\n",
        "data = generate_images_with_masks(total_size, image_size, min_radius, max_radius)\n",
        "\n",
        "# Define column names for the DataFrame: radius, x, y, and pixel values\n",
        "columns = ['radius', 'x', 'y'] + [f'Pixel_{i}' for i in range(image_size * image_size)]\n",
        "\n",
        "# Create a DataFrame from the generated data with the specified columns\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Save the DataFrame to a CSV file without including the index\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Notify the user that the data has been saved to the specified CSV file\n",
        "print(f\"Data saved to {csv_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj_3uhbJPODa"
      },
      "source": [
        "## Visual Exploration of Sample Data Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "2Y0ZCLfY9SZz"
      },
      "outputs": [],
      "source": [
        "# Function to visualize the image\n",
        "def visualize(image_data, image_size):\n",
        "    \"\"\"\n",
        "    Visualize the image given an array of image data and return the plot object.\n",
        "\n",
        "    Parameters:\n",
        "    - image_data: A NumPy array containing image metadata and mask.\n",
        "    - image_size: The size of the image (width and height, assumed square).\n",
        "\n",
        "    Returns:\n",
        "    - fig, ax: The matplotlib figure and axes objects for further modification.\n",
        "    \"\"\"\n",
        "    # Extract metadata from the array (radius and circle center coordinates)\n",
        "    radius = int(image_data[0])\n",
        "    x = int(image_data[1])\n",
        "    y = int(image_data[2])\n",
        "\n",
        "    # Extract the binary mask from the remaining part of the array\n",
        "    mask = image_data[3:].astype(np.uint8).reshape(image_size, image_size)\n",
        "\n",
        "    # Create a grayscale image from the mask\n",
        "    img = np.zeros((image_size, image_size), dtype=np.uint8)  # Initialize the image with zeros (black)\n",
        "    img[mask == 1] = 255  # Set the circle area to white (255)\n",
        "\n",
        "    # Create a figure and axis for plotting\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img, cmap='gray')  # Display the image in grayscale\n",
        "    ax.set_title(f\"Circle at center ({x}, {y}) with radius {radius}\")  # Set title with circle metadata\n",
        "    ax.axis('off')  # Turn off axis labels and ticks\n",
        "\n",
        "    # Return the figure and axis for further customization\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCtB1orRcubo"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_filename)\n",
        "\n",
        "# Prompt the user to enter an image index and extract the corresponding row\n",
        "#image_index = int(input(\"Enter the image index: \"))\n",
        "image_index = 536\n",
        "image_data = df.iloc[image_index].values  # Get the data for the specified image index\n",
        "\n",
        "# Visualize the selected image from the DataFrame\n",
        "_, _ = visualize(image_data, image_size)  # Call the visualize function to display the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut13k8BopY9D"
      },
      "source": [
        "# Section Two: Important functions\n",
        "Including OLS Solver with MSE and Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RnzSm6NvoHH5"
      },
      "outputs": [],
      "source": [
        "# Function to solve ordinary least squares (OLS)\n",
        "def solve_ols(X_train, Y_train, X_other, alpha):\n",
        "    \"\"\"\n",
        "    Solves the OLS regression problem with regularization (RLS).\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: The training input data (features).\n",
        "    - Y_train: The training output data (targets).\n",
        "    - X_other: The input data (features) for the other dataset (e.g., validation or test).\n",
        "    - alpha: Regularization parameter.\n",
        "\n",
        "    Returns:\n",
        "    - Y_LS_train: Predicted outputs for the training data.\n",
        "    - Y_LS_other: Predicted outputs for the other dataset.\n",
        "    \"\"\"\n",
        "    # Calculate the weights (W) using the pseudo-inverse and regularization\n",
        "    W = np.dot(np.linalg.pinv(np.dot(X_train.T, X_train) + alpha * np.identity(np.shape(X_train)[1])), np.dot(X_train.T, Y_train))\n",
        "\n",
        "\n",
        "    Y_LS_train = np.dot(X_train, W)\n",
        "    Y_LS_other = np.dot(X_other, W)\n",
        "    # Return predictions for both training and other data\n",
        "    return Y_LS_train, Y_LS_other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8nVlcBC4hqM0"
      },
      "outputs": [],
      "source": [
        "# Function to run OLS, compute MSE, and output results and labels\n",
        "def run_ols(X_train, X_other, Y_train, Y_other, alpha, augment=False):\n",
        "    \"\"\"\n",
        "    Runs the OLS regression, computes the MSE for both training and another dataset (e.g., validation/test), and returns results.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: The training input data (features).\n",
        "    - X_other: The input data (features) for the other dataset (e.g., validation or test).\n",
        "    - Y_train: The training output data (targets).\n",
        "    - Y_other: The output data (targets) for the other dataset (e.g., validation or test).\n",
        "    - alpha: Regularization parameter (ridge regression).\n",
        "    - augment: Boolean indicating whether to augment data by adding a bias term.\n",
        "\n",
        "    Returns:\n",
        "    - mse_train: Mean squared error on the training set.\n",
        "    - mse_other: Mean squared error on the other dataset.\n",
        "    - Y_LS_train: Predicted outputs for the training data.\n",
        "    - Y_LS_other: Predicted outputs for the other dataset.\n",
        "    \"\"\"\n",
        "    if augment:\n",
        "        # Augment the data by adding a bias term (column of ones)\n",
        "        X_train = augment_data(X_train)\n",
        "        X_other = augment_data(X_other)\n",
        "\n",
        "    # Perform OLS regression to get predictions\n",
        "    Y_LS_train, Y_LS_other = solve_ols(X_train, Y_train, X_other, alpha)\n",
        "\n",
        "    # Compute MSE for train and other datasets\n",
        "    mse_train = compute_mse(Y_LS_train, Y_train)\n",
        "    mse_other = compute_mse(Y_LS_other, Y_other)\n",
        "\n",
        "    # Return the MSE and predictions\n",
        "    return mse_train, mse_other, Y_LS_train, Y_LS_other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Eba3gdpEhvO5"
      },
      "outputs": [],
      "source": [
        "# Function to compute mean squared error (MSE)\n",
        "def compute_mse(Y_pred, Y_true):\n",
        "    \"\"\"\n",
        "    Computes the Mean Squared Error (MSE) between predicted and true values.\n",
        "\n",
        "    Parameters:\n",
        "    - Y_pred: Predicted values.\n",
        "    - Y_true: True values.\n",
        "\n",
        "    Returns:\n",
        "    - mse: The mean squared error.\n",
        "    \"\"\"\n",
        "    mse = np.square(np.linalg.norm(Y_pred - Y_true)) / Y_true.size\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "i58E89zjhxnQ"
      },
      "outputs": [],
      "source": [
        "# Function to augment data by adding a bias (for non-homogeneous models)\n",
        "def augment_data(X):\n",
        "    \"\"\"\n",
        "    Augments the data by adding a bias column (column of ones).\n",
        "\n",
        "    Parameters:\n",
        "    - X: The input data to augment.\n",
        "\n",
        "    Returns:\n",
        "    - Augmented input data with an added bias column.\n",
        "    \"\"\"\n",
        "    ones_column = np.ones((X.shape[0], 1))\n",
        "    return np.concatenate((X, ones_column), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "E24S2BzahztE"
      },
      "outputs": [],
      "source": [
        "# Function to report true and predicted circles\n",
        "def prediction_report(true_data, predicted_data, predicted_data_aug, index):\n",
        "    \"\"\"\n",
        "    Prints a report of true and predicted circle parameters (radius and center coordinates).\n",
        "\n",
        "    Parameters:\n",
        "    - true_data: Array containing true circle parameters (radius, x, y).\n",
        "    - predicted_data: Array containing predicted circle parameters using a homogeneous model.\n",
        "    - predicted_data_aug: Array containing predicted circle parameters using a non-homogeneous model.\n",
        "    - index: Index of the circle data to report.\n",
        "\n",
        "    Outputs:\n",
        "    - Prints the true and predicted values for the selected circle.\n",
        "    \"\"\"\n",
        "    # Extract true and predicted values for the selected index\n",
        "    true_radius, true_x, true_y = true_data[index]\n",
        "    pred_radius, pred_x, pred_y = predicted_data[index]\n",
        "    augp_radius, augp_x, augp_y = predicted_data_aug[index]\n",
        "\n",
        "    # Report the true and predicted values\n",
        "    print(f\"True values - Radius: {true_radius}, x: {true_x}, y: {true_y}\")\n",
        "    print(f\"Predicted homogeneous model values - Radius: {pred_radius:.2f}, x: {pred_x:.2f}, y: {pred_y:.2f}\")\n",
        "    print(f\"Predicted non-homogeneous model values - Radius: {augp_radius:.2f}, x: {augp_x:.2f}, y: {augp_y:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNDegoi7I1Rp"
      },
      "source": [
        "# Section Three: Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roajzvS5Ggbr"
      },
      "source": [
        "## PROBLEM 1 (20 marks): Predicting radius, x, and y using OLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "P8V93nfIHPJI"
      },
      "outputs": [],
      "source": [
        "# Load dataset from the CSV file\n",
        "df = pd.read_csv(csv_filename)  # Load the image data with metadata from a CSV file\n",
        "\n",
        "# Extract features by dropping the target columns ('radius', 'x', 'y')\n",
        "X = df.drop(columns=['radius', 'x', 'y']).values  # Features (only pixel data)\n",
        "\n",
        "# Extract target values (radius, x, and y)\n",
        "Y = df[['radius', 'x', 'y']].values  # Labels (radius, x, and y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmQRTNYVEmYi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### PART 1 (10 marks):\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. ### Divide the dataset into three parts: 1800 samples for training, 200 samples for validation, and 200 samples for testing. Perform linear OLS (without regularization) on the training samples twice—first with a homogeneous model (i.e., where the y-intercepts are zero) and then with a non-homogeneous model (allowing for a non-zero y-intercept). Report the MSE on both the training data and the validation data for each model.\n",
        "\n",
        "3. ### Compare the results. Which approach performs better? Why? Apply the better-performing approach to the test set and report the MSE.\n",
        "\n",
        "2. ### Do you observe significant overfitting in any of the cases?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QTEQgRSuHeiP"
      },
      "outputs": [],
      "source": [
        "# Specify the number of training, validation, testing samples.\n",
        "\n",
        "    ############################################################################\n",
        "    ############################################################################\n",
        "    #### Use the variable names \"train_size\", \"valid_size\", and \"test_size\" ####\n",
        "    #### for the training, validation, and test sizes, respectively.        ####\n",
        "    ############################################################################\n",
        "    ############################################################################\n",
        "\n",
        "train_size = 1800\n",
        "valid_size = 200\n",
        "test_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xrSPPiXhurQt"
      },
      "outputs": [],
      "source": [
        "def split_dataset(X, Y, total_size, train_size, valid_size):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Feature data.\n",
        "    Y (array-like): Target data.\n",
        "    total_size: Total number of data to consider.\n",
        "    train_size: number of the data to include in the training set.\n",
        "    valid_size: number of the data to include in the validation set.\n",
        "\n",
        "    Returns:\n",
        "    X_train, X_valid, X_test, Y_train, Y_valid, Y_test: Split datasets.\n",
        "\n",
        "    \"\"\"    \n",
        "\n",
        "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=train_size/total_size, random_state=42)\n",
        "    X_valid, X_test, Y_valid, Y_test = train_test_split(X_temp, Y_temp, train_size=valid_size/(total_size-train_size), random_state=42)\n",
        "    \n",
        "    return X_train, X_valid, X_test, Y_train, Y_valid, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XbXDYykHOIkh"
      },
      "outputs": [],
      "source": [
        "# Here we use the previous function to split our dataset into three parts.\n",
        "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = split_dataset(X, Y, total_size, train_size, valid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVVpEyqREXX8"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression without adding a bias term (homogeneous model)\n",
        "\n",
        "mse_train_homo, mse_other_homo, Y_LS_train_homo, Y_LS_other_homo = run_ols(X_train, X_valid, Y_train, Y_valid, alpha=0, augment=False)\n",
        "print(\"Homogeneous Model Training MSE:\", mse_train_homo)\n",
        "print(\"Homogeneous Model Validation MSE:\", mse_other_homo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gADWceHOGKI"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added (non-homogeneous model)\n",
        "\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_valid, Y_train, Y_valid, alpha=0, augment=True)\n",
        "\n",
        "# Output the results for the non-homogeneous model\n",
        "\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Validation MSE:\", mse_other_nh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztzdrN97AzcL"
      },
      "outputs": [],
      "source": [
        "# In this code you can see the predictions you've made using your model.\n",
        "\n",
        "# Prompt the user to enter an index for prediction reporting\n",
        "#index = int(input(\"Enter the index for prediction: \"))\n",
        "index = 45\n",
        "\n",
        "# Generate and print the prediction report for the specified index\n",
        "# Use the prediction_report function and index to see your predictions\n",
        "\n",
        "prediction_report(Y, Y_LS_train_homo, Y_LS_train_nh, index) \n",
        "prediction_report(Y, Y_LS_other_homo, Y_LS_other_nh, index) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2TUoETT6db-"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added on test set\n",
        "\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_test, Y_train, Y_test, alpha=0, augment=True)\n",
        "\n",
        "\n",
        "# Output the results for the better performing approach\n",
        "\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Test MSE:\", mse_other_nh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question answers in attached PDF file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LemqBWFFq5G"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### PART 2 (10 marks):\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. ### Divide the dataset into three parts: 200 samples for training, 1800 samples for validation, and 200 samples for testing. Perform linear OLS (without regularization) on the training samples twice—first with a homogeneous model (i.e., where the y-intercepts are zero) and then with a non-homogeneous model (allowing for a non-zero y-intercept). Report the MSE on both the training data and the validation data for each model.\n",
        "\n",
        "2. ### Compare these results with those from the previous part. Do you observe less overfitting or more overfitting? How did you arrive at this conclusion?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NAUFgyKqHla4"
      },
      "outputs": [],
      "source": [
        "# Specify the number of training, validation, testing samples.\n",
        "\n",
        "train_size = 200\n",
        "valid_size = 1800\n",
        "test_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "FmTeeS5fPMny"
      },
      "outputs": [],
      "source": [
        "# Here we use the split_dataset function to split our dataset into three parts.\n",
        "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = split_dataset(X, Y, total_size, train_size, valid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXEfgyU1PXzP"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression without adding a bias term (homogeneous model)\n",
        "\n",
        "mse_train_homo, mse_other_homo, Y_LS_train_homo, Y_LS_other_homo = run_ols(X_train, X_valid, Y_train, Y_valid, alpha=0, augment=False)\n",
        "\n",
        "# Output the results for the homogeneous model\n",
        "\n",
        "    ########################\n",
        "print(\"Homogeneous Model Training MSE:\", mse_train_homo)\n",
        "print(\"Homogeneous Model Validation MSE:\", mse_other_homo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn2M609vPZ94"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added (non-homogeneous model)\n",
        "\n",
        "\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_valid, Y_train, Y_valid, alpha=0, augment=True)\n",
        "\n",
        "# Output the results for the non-homogeneous model\n",
        "\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Validation MSE:\", mse_other_nh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aka2smhQ0jBo"
      },
      "outputs": [],
      "source": [
        "# In this code you can see the predictions you've made using your model.\n",
        "\n",
        "# Prompt the user to enter an index for prediction reporting\n",
        "#index = int(input(\"Enter the index for prediction: \"))\n",
        "index = 45\n",
        "# Generate and print the prediction report for the specified index\n",
        "# Use the prediction_report function and index to see your predictions\n",
        "\n",
        "\n",
        "prediction_report(Y, Y_LS_train_homo, Y_LS_train_nh, index) \n",
        "prediction_report(Y, Y_LS_other_homo, Y_LS_other_nh, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8HnRa8T7uIX"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added on test set\n",
        "\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_test, Y_train, Y_test, alpha=0, augment=True)\n",
        "\n",
        "\n",
        "# Output the results for the better performing approach\n",
        "\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Test MSE:\", mse_other_nh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question answers in attached PDF file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQCxA56yG7LH"
      },
      "source": [
        "## PROBLEM 2 (20 marks): Regularized Least Squares\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "LkvOHV4UHH3s"
      },
      "outputs": [],
      "source": [
        "# Load dataset from the CSV file\n",
        "df = pd.read_csv(csv_filename)  # Load the image data with metadata from a CSV file\n",
        "\n",
        "# Extract features by dropping the target columns ('radius', 'x', 'y')\n",
        "X = df.drop(columns=['radius', 'x', 'y']).values  # Features (only pixel data)\n",
        "\n",
        "# Extract target values (radius, x, and y)\n",
        "Y = df[['radius', 'x', 'y']].values  # Labels (radius, x, and y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bUV0trHzgW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### PART 1 (15 marks):\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "1. ### Divide the Dataset into Three Parts:\n",
        "- **Training Data**: Select **200 data points**.\n",
        "- **Validation Data**: Assign **1800 data points**.\n",
        "- **Testing Data**: Set aside the **remaining 200 data points** for testing.\n",
        "\n",
        "\n",
        "2. ### Run Regularized Least Squares (non-homogeneous) using 200 training data points. Choose various values of lambda within the range {exp(-2), exp(-1.5), exp(-1), …, exp(3.5), exp(4)}. This corresponds to λ values ranging from exp(-2) to exp(4) with a step size of 0.5. For each value of lambda, Run Regularized Least Squares (non-homogeneous) using 200 training data points. Compute the Training MSE and Validation MSE.\n",
        "\n",
        "\n",
        "3. ### Plot the Training MSE and Validation MSE as functions of lambda.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **IMPORTANT NOTE:** Use the variable name \"alpha\" for the values of the regularization parameter (previously referred to as \"lambda\") in Python code, since \"lambda\" is a reserved keyword in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "YPAVI7m18Ggv"
      },
      "outputs": [],
      "source": [
        "# Specify the number of training, validation, testing samples.\n",
        "\n",
        "train_size = 200\n",
        "valid_size = 1800\n",
        "test_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "SuuzU8cZ8H5w"
      },
      "outputs": [],
      "source": [
        "# Here we use the split_dataset function to split our dataset into three parts.\n",
        "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = split_dataset(X, Y, total_size, train_size, valid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8tHdr-RHyen"
      },
      "outputs": [],
      "source": [
        "# Define a range of alpha values (regularization parameters) using an exponential scale\n",
        "\n",
        "alphas = np.exp(np.arange(-2, 4.5, 0.5))\n",
        "\n",
        "validation_mses = []\n",
        "training_mses = []\n",
        "# Iterate over the range of alpha values\n",
        "for alpha in alphas:\n",
        "\n",
        "  # Run RLS regression with a bias term added on train and validation sets\n",
        "\n",
        "  mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_valid, Y_train, Y_valid, alpha, augment=True)\n",
        "  training_mses.append(mse_train_nh)\n",
        "  validation_mses.append(mse_other_nh)\n",
        "\n",
        "\n",
        "# Plot the MSE for the non-homogeneous model across different alphas\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alphas, training_mses, label='Training MSE', marker='o')\n",
        "plt.plot(alphas, validation_mses, label='Validation MSE', marker='o')\n",
        "\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Training and Validation MSE vs Alpha')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(\"Lambda value with minimum validation MSE:\", alphas[np.argmin(validation_mses)])\n",
        "print(min(validation_mses))\n",
        "print(\"Training MSEs\", training_mses)\n",
        "print(\"Validation MSEs\", validation_mses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pJ6sFasJMsp"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### PART 2 (5 marks):\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1. ### What is the best value for lambda? Why?\n",
        "\n",
        "2. ### Use the best value of lambda to report the results on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question answers in attached PDF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H00qLoDt-vjh"
      },
      "outputs": [],
      "source": [
        "# Here you find the best value of lambda\n",
        "\n",
        "best_alpha = alphas[np.argmin(validation_mses)]\n",
        "print(\"Lambda value with minimum validation MSE:\", best_alpha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKPn0yjm-6tb"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added (non-homogeneous model)\n",
        "\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_test, Y_train, Y_test, best_alpha, augment=True)\n",
        "\n",
        "# Output the results for the non-homogeneous model\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Test MSE:\", mse_other_nh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3vjcNr0p0Q6"
      },
      "source": [
        "## PROBLEM 3 (30 marks): Preprocessing the data\n",
        "\n",
        "\n",
        "### In this question, we aim to improve the results of Regularized Least Squares (RLS) while still using only 200 training points and 1800 validation points. To achieve this, we will map the data points into a new space and then run RLS. In other words, we will preprocess X_train, X_valid, and X_test to obtain new Z_train, Z_valid, and Z_test datasets, which will have the same number of points but a different number of features.\n",
        "\n",
        "### NOTE: You will only get full mark in this question if your approach improves over the naive use of RLS we observed in the previous questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrmVcW2J_fe"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "### PART 1 (5 marks):\n",
        "---\n",
        "---\n",
        "\n",
        " - ## Choose a preprocessing approach (i.e., select a mapping) that transforms the 900-dimensional data points (900 pixels) into a new space. This new space can be either lower-dimensional or higher-dimensional. Clearly explain your preprocessing approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Question answers in attached PDF file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMjRrE4PJjWE"
      },
      "source": [
        "---\n",
        "---\n",
        "### PART 2 (10 marks):\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "- ## Implement your preprocessing approach. Then, run non-homogeneous Regularized Least Squares (RLS) in the new space for training set and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from the CSV file\n",
        "df = pd.read_csv(csv_filename)  # Load the image data with metadata from a CSV file\n",
        "\n",
        "# Extract features by dropping the target columns ('radius', 'x', 'y')\n",
        "X = df.drop(columns=['radius', 'x', 'y']).values  # Features (only pixel data)\n",
        "\n",
        "# Extract target values (radius, x, and y)\n",
        "Y = df[['radius', 'x', 'y']].values  # Labels (radius, x, and y)\n",
        "\n",
        "# Define a function to transforms pixel values to a new space.\n",
        "def transform(X_df):\n",
        "    processed_X_df = []\n",
        "    for image in X_df:\n",
        "        circle_border_pixels = []\n",
        "        for i in range(0, image_size*image_size, image_size):\n",
        "            curr_row = image[i:i+image_size]\n",
        "            left_outer = 0\n",
        "            right_outer = 0\n",
        "            if np.any(curr_row != 0):\n",
        "                left_outer = np.argmax(curr_row != 0)\n",
        "                right_outer = (image_size - 1) - np.argmax(curr_row[::-1] != 0)\n",
        "            circle_border_pixels.extend([left_outer,right_outer])\n",
        "        processed_X_df.append(circle_border_pixels)\n",
        "\n",
        "    \n",
        "    return np.array(processed_X_df)\n",
        "\n",
        "# Apply the transform function to the DataFrame\n",
        "processed_X_df = transform(X)\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n",
        "# processed_X_df = pd.DataFrame(processed_X_df)\n",
        "# processed_X_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq8QgowJwriU"
      },
      "source": [
        "---\n",
        "---\n",
        "### PART 3 (15 marks):\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "- ## Report the MSE on the training and validation sets for different values of lambda and plot it. **As mentioned, it should perform better for getting points.** choose the best value of lambda, apply your preprocessing approach to the test set, and then report the MSE after running RLS.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "c5hTDDY8R8sq"
      },
      "outputs": [],
      "source": [
        "# Specify the number of training, validation, testing samples.\n",
        "\n",
        "train_size = 200\n",
        "valid_size = 1800\n",
        "test_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "735hxlxUR_GJ"
      },
      "outputs": [],
      "source": [
        "########################################################\n",
        "########################################################\n",
        "#### Please take note that we insert processed_X_df ####\n",
        "#### as the new input for splitting                 ####\n",
        "########################################################\n",
        "########################################################\n",
        "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = split_dataset(processed_X_df, Y, total_size, train_size, valid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOzgptq5SNOf"
      },
      "outputs": [],
      "source": [
        "# Define a range of alpha values (regularization parameters) using an exponential scale\n",
        "alphas = np.exp(np.arange(-5, 5, 0.5))\n",
        "\n",
        "validation_mses = []\n",
        "training_mses = []\n",
        "# Iterate over the range of alpha values\n",
        "for alpha in alphas:\n",
        "  mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_valid, Y_train, Y_valid, alpha, augment=True)\n",
        "  training_mses.append(mse_train_nh)\n",
        "  validation_mses.append(mse_other_nh)\n",
        "\n",
        "df_data = pd.DataFrame({\n",
        "    'Alpha': alphas,\n",
        "    'Training MSE': training_mses,\n",
        "    'Validation MSE': validation_mses\n",
        "})\n",
        "\n",
        "display(df_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kxj3b-HuAM8"
      },
      "outputs": [],
      "source": [
        "# Plot the MSE for the non-homogeneous model across different alphas\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(alphas, training_mses, label='Training MSE', marker='o')\n",
        "plt.plot(alphas, validation_mses, label='Validation MSE', marker='o')\n",
        "\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Training and Validation MSE vs Alpha')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(\"Lambda value with minimum validation MSE:\", alphas[np.argmin(validation_mses)])\n",
        "print(min(validation_mses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF4SS7GoBmMU"
      },
      "outputs": [],
      "source": [
        "# Here you find the best value of lambda\n",
        "\n",
        "print(\"Lambda value with minimum validation MSE:\", alphas[np.argmin(validation_mses)])\n",
        "print(validation_mses)\n",
        "best_alpha = alphas[np.argmin(validation_mses)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added (non-homogeneous model)\n",
        "\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_valid, Y_train, Y_valid, best_alpha, augment=True)\n",
        "\n",
        "# Output the results for the non-homogeneous model\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Validation MSE:\", mse_other_nh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUoS9rFJxOC0"
      },
      "outputs": [],
      "source": [
        "# Run OLS regression with a bias term added (non-homogeneous model)\n",
        "mse_train_nh, mse_other_nh, Y_LS_train_nh, Y_LS_other_nh = run_ols(X_train, X_test, Y_train, Y_test, best_alpha, augment=True)\n",
        "\n",
        "# Output the results for the non-homogeneous model\n",
        "print(\"Non-Homogeneous Model Training MSE:\", mse_train_nh)\n",
        "print(\"Non-Homogeneous Model Test MSE:\", mse_other_nh)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
