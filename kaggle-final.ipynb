{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak-hannou/compsci-4ml3/blob/main/kaggle-final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dga32YQxm-vV"
      },
      "outputs": [],
      "source": [
        "import torch  # Core PyTorch library for tensor operations\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2  # Datasets and transformations for computer vision\n",
        "import torch.nn as nn  # Neural network components\n",
        "import torch.optim as optim  # Optimization algorithms\n",
        "import numpy as np  # Numerical operations\n",
        "import matplotlib.pyplot as plt  # Plotting\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset, TensorDataset  # Data handling utilities\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 #\n",
        "learning_rate = 0.0001      # Learning rate for optimizer\n",
        "num_epochs = 250"
      ],
      "metadata": {
        "id": "jGLOKUnUwYwo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    # v2.RandomZoomOut(),\n",
        "    v2.RandomRotation(20),\n",
        "    # v2.RandomResizedCrop(32),\n",
        "      v2.ToTensor(),\n",
        "      v2.Normalize(cifar100_mean, cifar100_std),\n",
        "      ###### Add your transformations here ########\n",
        "])\n",
        "test_transform = v2.Compose([\n",
        "      v2.ToTensor(),\n",
        "      v2.Normalize(cifar100_mean, cifar100_std),\n",
        "      ###### Add your transformations here ########\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR100(\n",
        "    root='./data',       # Change this path if needed\n",
        "    train=True,          # Set to True to download the training set\n",
        "    download=True,       # Set to True to download if not already downloaded\n",
        "    transform=train_transform  # Apply transformations\n",
        ")\n",
        "test_dataset = datasets.CIFAR100(\n",
        "    root='./data',       # Change this path if needed\n",
        "    train=False,          # Set to True to download the training set\n",
        "    download=True,       # Set to True to download if not already downloaded\n",
        "    transform=test_transform  # Apply transformations\n",
        ")\n",
        "##### Hyper-parameters\n",
        "\n",
        "\n",
        "# Create a DataLoader for batch processing\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,      # Batch size (you can modify this as needed)\n",
        "    shuffle=True        # Shuffle data for training\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,      # Batch size (you can modify this as needed)\n",
        "    shuffle=False        # Shuffle data for training\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvm1Cjngv7Pr",
        "outputId": "b7be20fa-d6e5-4153-b6f9-127d3179337c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(CIFARClassifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, padding=1)\n",
        "        #self.batch_norm1 = nn.BatchNorm2d(128)\n",
        "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        #self.batch_norm2 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc1 = nn.Linear(16384, 512)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        self.batch_norm = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 100)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "  def forward(self, x):\n",
        "        x = self.relu((self.conv1(x)))\n",
        "        x = self.relu((self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.relu((self.conv3(x)))\n",
        "        x = self.relu((self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BuxC_N-PxIvs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CIFARClassifier().to(device)\n",
        "print(device)\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy includes softmax\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Xl1e1dj5xMDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57094d00-ad0d-487b-b7b8-724bb8c5a607"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss_batches = []\n",
        "\n",
        "  # Switch to evaluation mode and turn off gradient calculation\n",
        "  # since parameters are not updated during testing.\n",
        "  with torch.no_grad():\n",
        "      for images_batch, labels_batch in data_loader:\n",
        "          images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "          outputs = model(images_batch) # Forward pass\n",
        "          # The predicted label is the output with the highest activation.\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels_batch.size(0)\n",
        "          correct += (predicted == labels_batch).sum().item()\n",
        "\n",
        "          # Use provided criterion to calculate the loss for the mini batch\n",
        "          # Append the mini-batch loss to loss_batches array\n",
        "          batch_loss = criterion(outputs, labels_batch)\n",
        "          loss_batches.append(batch_loss.item())\n",
        "\n",
        "      accuracy = 100 * correct / total\n",
        "      avg_loss = np.mean(loss_batches)\n",
        "\n",
        "      model.train()\n",
        "\n",
        "\n",
        "      return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "f81I9sMXxPL7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.train()\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images_batch, labels_batch) in enumerate(train_loader):\n",
        "        optimizer.zero_grad() # Clear the gradients\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "        outputs = model(images_batch) # Forward pass\n",
        "        loss = criterion(outputs, labels_batch) # Calculate loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "    # Evaluate on train and test sets after each epoch\n",
        "\n",
        "    train_accuracy, train_loss = evaluate(model, train_loader, criterion)\n",
        "    test_accuracy, test_loss = evaluate(model, test_loader, criterion)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1:02d}/{num_epochs:02d} - Train Loss: {train_loss:.6f}, Train Acc: {train_accuracy:.2f}%')\n",
        "    print(f'            - Test Loss: {test_loss:.6f}, Test Acc: {test_accuracy:.2f}%')\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UxXh0LmxSnI",
        "outputId": "e1f060df-0cd8-4f4c-aca0-03924a7c235a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/250 - Train Loss: 3.312363, Train Acc: 23.24%\n",
            "            - Test Loss: 3.300554, Test Acc: 23.86%\n",
            "------------------------------------------------------------\n",
            "Epoch 02/250 - Train Loss: 2.865386, Train Acc: 31.12%\n",
            "            - Test Loss: 2.838215, Test Acc: 31.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 03/250 - Train Loss: 2.350265, Train Acc: 40.78%\n",
            "            - Test Loss: 2.379840, Test Acc: 39.79%\n",
            "------------------------------------------------------------\n",
            "Epoch 04/250 - Train Loss: 2.045105, Train Acc: 48.47%\n",
            "            - Test Loss: 2.159988, Test Acc: 45.28%\n",
            "------------------------------------------------------------\n",
            "Epoch 05/250 - Train Loss: 1.845189, Train Acc: 51.56%\n",
            "            - Test Loss: 1.995529, Test Acc: 48.28%\n",
            "------------------------------------------------------------\n",
            "Epoch 06/250 - Train Loss: 1.617640, Train Acc: 57.38%\n",
            "            - Test Loss: 1.862278, Test Acc: 50.77%\n",
            "------------------------------------------------------------\n",
            "Epoch 07/250 - Train Loss: 1.506165, Train Acc: 60.14%\n",
            "            - Test Loss: 1.810875, Test Acc: 52.08%\n",
            "------------------------------------------------------------\n",
            "Epoch 08/250 - Train Loss: 1.400343, Train Acc: 63.11%\n",
            "            - Test Loss: 1.767966, Test Acc: 52.88%\n",
            "------------------------------------------------------------\n",
            "Epoch 09/250 - Train Loss: 1.318507, Train Acc: 64.56%\n",
            "            - Test Loss: 1.767083, Test Acc: 53.45%\n",
            "------------------------------------------------------------\n",
            "Epoch 10/250 - Train Loss: 1.181912, Train Acc: 69.17%\n",
            "            - Test Loss: 1.682384, Test Acc: 55.43%\n",
            "------------------------------------------------------------\n",
            "Epoch 11/250 - Train Loss: 1.099444, Train Acc: 70.22%\n",
            "            - Test Loss: 1.685899, Test Acc: 55.20%\n",
            "------------------------------------------------------------\n",
            "Epoch 12/250 - Train Loss: 1.030265, Train Acc: 72.87%\n",
            "            - Test Loss: 1.660930, Test Acc: 55.93%\n",
            "------------------------------------------------------------\n",
            "Epoch 13/250 - Train Loss: 0.930289, Train Acc: 75.46%\n",
            "            - Test Loss: 1.633834, Test Acc: 56.77%\n",
            "------------------------------------------------------------\n",
            "Epoch 14/250 - Train Loss: 0.849772, Train Acc: 77.67%\n",
            "            - Test Loss: 1.642772, Test Acc: 57.22%\n",
            "------------------------------------------------------------\n",
            "Epoch 15/250 - Train Loss: 0.785988, Train Acc: 78.96%\n",
            "            - Test Loss: 1.625764, Test Acc: 57.42%\n",
            "------------------------------------------------------------\n",
            "Epoch 16/250 - Train Loss: 0.844280, Train Acc: 76.49%\n",
            "            - Test Loss: 1.741365, Test Acc: 56.33%\n",
            "------------------------------------------------------------\n",
            "Epoch 17/250 - Train Loss: 0.842722, Train Acc: 76.90%\n",
            "            - Test Loss: 1.822111, Test Acc: 54.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 18/250 - Train Loss: 0.625761, Train Acc: 83.58%\n",
            "            - Test Loss: 1.630722, Test Acc: 57.93%\n",
            "------------------------------------------------------------\n",
            "Epoch 19/250 - Train Loss: 0.613640, Train Acc: 83.72%\n",
            "            - Test Loss: 1.665906, Test Acc: 57.42%\n",
            "------------------------------------------------------------\n",
            "Epoch 20/250 - Train Loss: 0.577482, Train Acc: 84.59%\n",
            "            - Test Loss: 1.649162, Test Acc: 57.98%\n",
            "------------------------------------------------------------\n",
            "Epoch 21/250 - Train Loss: 0.668068, Train Acc: 81.41%\n",
            "            - Test Loss: 1.803389, Test Acc: 56.12%\n",
            "------------------------------------------------------------\n",
            "Epoch 22/250 - Train Loss: 0.549965, Train Acc: 85.62%\n",
            "            - Test Loss: 1.688512, Test Acc: 57.93%\n",
            "------------------------------------------------------------\n",
            "Epoch 23/250 - Train Loss: 0.536105, Train Acc: 85.76%\n",
            "            - Test Loss: 1.704179, Test Acc: 57.65%\n",
            "------------------------------------------------------------\n",
            "Epoch 24/250 - Train Loss: 0.648794, Train Acc: 82.10%\n",
            "            - Test Loss: 1.888747, Test Acc: 53.97%\n",
            "------------------------------------------------------------\n",
            "Epoch 25/250 - Train Loss: 0.406537, Train Acc: 89.75%\n",
            "            - Test Loss: 1.697603, Test Acc: 57.86%\n",
            "------------------------------------------------------------\n",
            "Epoch 26/250 - Train Loss: 0.345450, Train Acc: 91.31%\n",
            "            - Test Loss: 1.677387, Test Acc: 59.10%\n",
            "------------------------------------------------------------\n",
            "Epoch 27/250 - Train Loss: 0.521380, Train Acc: 85.26%\n",
            "            - Test Loss: 1.956113, Test Acc: 55.40%\n",
            "------------------------------------------------------------\n",
            "Epoch 28/250 - Train Loss: 0.288339, Train Acc: 93.27%\n",
            "            - Test Loss: 1.660505, Test Acc: 59.27%\n",
            "------------------------------------------------------------\n",
            "Epoch 29/250 - Train Loss: 0.342481, Train Acc: 90.93%\n",
            "            - Test Loss: 1.746842, Test Acc: 58.48%\n",
            "------------------------------------------------------------\n",
            "Epoch 30/250 - Train Loss: 0.258866, Train Acc: 93.56%\n",
            "            - Test Loss: 1.694697, Test Acc: 59.81%\n",
            "------------------------------------------------------------\n",
            "Epoch 31/250 - Train Loss: 0.285039, Train Acc: 92.92%\n",
            "            - Test Loss: 1.785951, Test Acc: 57.45%\n",
            "------------------------------------------------------------\n",
            "Epoch 32/250 - Train Loss: 0.355419, Train Acc: 90.33%\n",
            "            - Test Loss: 1.830803, Test Acc: 57.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 33/250 - Train Loss: 0.226427, Train Acc: 94.45%\n",
            "            - Test Loss: 1.742740, Test Acc: 59.19%\n",
            "------------------------------------------------------------\n",
            "Epoch 34/250 - Train Loss: 0.272948, Train Acc: 92.49%\n",
            "            - Test Loss: 1.871647, Test Acc: 58.16%\n",
            "------------------------------------------------------------\n",
            "Epoch 35/250 - Train Loss: 0.216229, Train Acc: 94.81%\n",
            "            - Test Loss: 1.745904, Test Acc: 58.73%\n",
            "------------------------------------------------------------\n",
            "Epoch 36/250 - Train Loss: 0.167760, Train Acc: 96.14%\n",
            "            - Test Loss: 1.732397, Test Acc: 59.88%\n",
            "------------------------------------------------------------\n",
            "Epoch 37/250 - Train Loss: 0.170526, Train Acc: 96.02%\n",
            "            - Test Loss: 1.799341, Test Acc: 58.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 38/250 - Train Loss: 0.164873, Train Acc: 95.73%\n",
            "            - Test Loss: 1.838632, Test Acc: 58.93%\n",
            "------------------------------------------------------------\n",
            "Epoch 39/250 - Train Loss: 0.217106, Train Acc: 94.74%\n",
            "            - Test Loss: 1.835637, Test Acc: 57.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 40/250 - Train Loss: 0.144744, Train Acc: 96.92%\n",
            "            - Test Loss: 1.782554, Test Acc: 59.20%\n",
            "------------------------------------------------------------\n",
            "Epoch 41/250 - Train Loss: 0.152399, Train Acc: 96.21%\n",
            "            - Test Loss: 1.843245, Test Acc: 59.37%\n",
            "------------------------------------------------------------\n",
            "Epoch 42/250 - Train Loss: 0.137889, Train Acc: 96.69%\n",
            "            - Test Loss: 1.887786, Test Acc: 58.40%\n",
            "------------------------------------------------------------\n",
            "Epoch 43/250 - Train Loss: 0.127367, Train Acc: 96.97%\n",
            "            - Test Loss: 1.849486, Test Acc: 59.25%\n",
            "------------------------------------------------------------\n",
            "Epoch 44/250 - Train Loss: 0.137836, Train Acc: 96.74%\n",
            "            - Test Loss: 1.860802, Test Acc: 59.13%\n",
            "------------------------------------------------------------\n",
            "Epoch 45/250 - Train Loss: 0.152143, Train Acc: 96.59%\n",
            "            - Test Loss: 1.824239, Test Acc: 58.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 46/250 - Train Loss: 0.133324, Train Acc: 96.85%\n",
            "            - Test Loss: 1.882640, Test Acc: 58.65%\n",
            "------------------------------------------------------------\n",
            "Epoch 47/250 - Train Loss: 0.126734, Train Acc: 97.01%\n",
            "            - Test Loss: 1.854916, Test Acc: 58.74%\n",
            "------------------------------------------------------------\n",
            "Epoch 48/250 - Train Loss: 0.098768, Train Acc: 97.77%\n",
            "            - Test Loss: 1.874178, Test Acc: 59.83%\n",
            "------------------------------------------------------------\n",
            "Epoch 49/250 - Train Loss: 0.104694, Train Acc: 97.66%\n",
            "            - Test Loss: 1.852879, Test Acc: 59.63%\n",
            "------------------------------------------------------------\n",
            "Epoch 50/250 - Train Loss: 0.103860, Train Acc: 97.43%\n",
            "            - Test Loss: 1.980685, Test Acc: 58.56%\n",
            "------------------------------------------------------------\n",
            "Epoch 51/250 - Train Loss: 0.084080, Train Acc: 98.22%\n",
            "            - Test Loss: 1.870768, Test Acc: 59.66%\n",
            "------------------------------------------------------------\n",
            "Epoch 52/250 - Train Loss: 0.112594, Train Acc: 97.32%\n",
            "            - Test Loss: 1.911182, Test Acc: 59.03%\n",
            "------------------------------------------------------------\n",
            "Epoch 53/250 - Train Loss: 0.093337, Train Acc: 97.72%\n",
            "            - Test Loss: 2.009864, Test Acc: 59.55%\n",
            "------------------------------------------------------------\n",
            "Epoch 54/250 - Train Loss: 0.084620, Train Acc: 98.11%\n",
            "            - Test Loss: 1.939043, Test Acc: 58.98%\n",
            "------------------------------------------------------------\n",
            "Epoch 55/250 - Train Loss: 0.104361, Train Acc: 97.77%\n",
            "            - Test Loss: 1.962272, Test Acc: 58.22%\n",
            "------------------------------------------------------------\n",
            "Epoch 56/250 - Train Loss: 0.057730, Train Acc: 98.90%\n",
            "            - Test Loss: 1.932731, Test Acc: 59.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 57/250 - Train Loss: 0.199231, Train Acc: 94.58%\n",
            "            - Test Loss: 2.109851, Test Acc: 56.28%\n",
            "------------------------------------------------------------\n",
            "Epoch 58/250 - Train Loss: 0.059722, Train Acc: 98.77%\n",
            "            - Test Loss: 1.937449, Test Acc: 59.89%\n",
            "------------------------------------------------------------\n",
            "Epoch 59/250 - Train Loss: 0.060635, Train Acc: 98.82%\n",
            "            - Test Loss: 1.954466, Test Acc: 58.95%\n",
            "------------------------------------------------------------\n",
            "Epoch 60/250 - Train Loss: 0.075685, Train Acc: 98.24%\n",
            "            - Test Loss: 2.000827, Test Acc: 58.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 61/250 - Train Loss: 0.094700, Train Acc: 97.66%\n",
            "            - Test Loss: 2.049802, Test Acc: 58.70%\n",
            "------------------------------------------------------------\n",
            "Epoch 62/250 - Train Loss: 0.077173, Train Acc: 98.17%\n",
            "            - Test Loss: 2.028994, Test Acc: 59.63%\n",
            "------------------------------------------------------------\n",
            "Epoch 63/250 - Train Loss: 0.065969, Train Acc: 98.38%\n",
            "            - Test Loss: 2.062975, Test Acc: 59.66%\n",
            "------------------------------------------------------------\n",
            "Epoch 64/250 - Train Loss: 0.105224, Train Acc: 97.36%\n",
            "            - Test Loss: 2.115874, Test Acc: 57.13%\n",
            "------------------------------------------------------------\n",
            "Epoch 65/250 - Train Loss: 0.063758, Train Acc: 98.53%\n",
            "            - Test Loss: 2.013081, Test Acc: 59.50%\n",
            "------------------------------------------------------------\n",
            "Epoch 66/250 - Train Loss: 0.045997, Train Acc: 99.02%\n",
            "            - Test Loss: 2.014459, Test Acc: 59.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 67/250 - Train Loss: 0.059311, Train Acc: 98.75%\n",
            "            - Test Loss: 1.993333, Test Acc: 59.15%\n",
            "------------------------------------------------------------\n",
            "Epoch 68/250 - Train Loss: 0.045430, Train Acc: 99.06%\n",
            "            - Test Loss: 2.045650, Test Acc: 59.88%\n",
            "------------------------------------------------------------\n",
            "Epoch 69/250 - Train Loss: 0.052287, Train Acc: 98.83%\n",
            "            - Test Loss: 2.034684, Test Acc: 60.16%\n",
            "------------------------------------------------------------\n",
            "Epoch 70/250 - Train Loss: 0.057332, Train Acc: 98.73%\n",
            "            - Test Loss: 2.036054, Test Acc: 59.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 71/250 - Train Loss: 0.059353, Train Acc: 98.58%\n",
            "            - Test Loss: 2.100023, Test Acc: 59.32%\n",
            "------------------------------------------------------------\n",
            "Epoch 72/250 - Train Loss: 0.044718, Train Acc: 98.93%\n",
            "            - Test Loss: 2.158152, Test Acc: 59.69%\n",
            "------------------------------------------------------------\n",
            "Epoch 73/250 - Train Loss: 0.056532, Train Acc: 98.63%\n",
            "            - Test Loss: 2.122089, Test Acc: 58.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 74/250 - Train Loss: 0.038900, Train Acc: 99.21%\n",
            "            - Test Loss: 2.002400, Test Acc: 59.90%\n",
            "------------------------------------------------------------\n",
            "Epoch 75/250 - Train Loss: 0.042199, Train Acc: 99.14%\n",
            "            - Test Loss: 2.070748, Test Acc: 60.11%\n",
            "------------------------------------------------------------\n",
            "Epoch 76/250 - Train Loss: 0.034053, Train Acc: 99.39%\n",
            "            - Test Loss: 2.011470, Test Acc: 60.21%\n",
            "------------------------------------------------------------\n",
            "Epoch 77/250 - Train Loss: 0.057751, Train Acc: 98.56%\n",
            "            - Test Loss: 2.125341, Test Acc: 59.61%\n",
            "------------------------------------------------------------\n",
            "Epoch 78/250 - Train Loss: 0.169666, Train Acc: 94.86%\n",
            "            - Test Loss: 2.417401, Test Acc: 56.81%\n",
            "------------------------------------------------------------\n",
            "Epoch 79/250 - Train Loss: 0.029270, Train Acc: 99.42%\n",
            "            - Test Loss: 2.068815, Test Acc: 60.42%\n",
            "------------------------------------------------------------\n",
            "Epoch 80/250 - Train Loss: 0.034392, Train Acc: 99.20%\n",
            "            - Test Loss: 2.119082, Test Acc: 60.05%\n",
            "------------------------------------------------------------\n",
            "Epoch 81/250 - Train Loss: 0.033495, Train Acc: 99.39%\n",
            "            - Test Loss: 2.090886, Test Acc: 59.44%\n",
            "------------------------------------------------------------\n",
            "Epoch 82/250 - Train Loss: 0.041048, Train Acc: 99.14%\n",
            "            - Test Loss: 2.112777, Test Acc: 59.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 83/250 - Train Loss: 0.034821, Train Acc: 99.17%\n",
            "            - Test Loss: 2.152951, Test Acc: 59.79%\n",
            "------------------------------------------------------------\n",
            "Epoch 84/250 - Train Loss: 0.034048, Train Acc: 99.29%\n",
            "            - Test Loss: 2.097919, Test Acc: 59.83%\n",
            "------------------------------------------------------------\n",
            "Epoch 85/250 - Train Loss: 0.042336, Train Acc: 99.06%\n",
            "            - Test Loss: 2.171691, Test Acc: 58.71%\n",
            "------------------------------------------------------------\n",
            "Epoch 86/250 - Train Loss: 0.033278, Train Acc: 99.30%\n",
            "            - Test Loss: 2.115819, Test Acc: 59.73%\n",
            "------------------------------------------------------------\n",
            "Epoch 87/250 - Train Loss: 0.057827, Train Acc: 98.43%\n",
            "            - Test Loss: 2.269111, Test Acc: 58.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 88/250 - Train Loss: 0.035338, Train Acc: 99.26%\n",
            "            - Test Loss: 2.141865, Test Acc: 59.07%\n",
            "------------------------------------------------------------\n",
            "Epoch 89/250 - Train Loss: 0.038661, Train Acc: 99.05%\n",
            "            - Test Loss: 2.183356, Test Acc: 59.40%\n",
            "------------------------------------------------------------\n",
            "Epoch 90/250 - Train Loss: 0.077606, Train Acc: 97.78%\n",
            "            - Test Loss: 2.332632, Test Acc: 58.63%\n",
            "------------------------------------------------------------\n",
            "Epoch 91/250 - Train Loss: 0.023904, Train Acc: 99.54%\n",
            "            - Test Loss: 2.128965, Test Acc: 60.00%\n",
            "------------------------------------------------------------\n",
            "Epoch 92/250 - Train Loss: 0.047244, Train Acc: 98.91%\n",
            "            - Test Loss: 2.236655, Test Acc: 58.27%\n",
            "------------------------------------------------------------\n",
            "Epoch 93/250 - Train Loss: 0.035703, Train Acc: 99.28%\n",
            "            - Test Loss: 2.147243, Test Acc: 59.89%\n",
            "------------------------------------------------------------\n",
            "Epoch 94/250 - Train Loss: 0.031742, Train Acc: 99.19%\n",
            "            - Test Loss: 2.282126, Test Acc: 60.08%\n",
            "------------------------------------------------------------\n",
            "Epoch 95/250 - Train Loss: 0.025368, Train Acc: 99.45%\n",
            "            - Test Loss: 2.157496, Test Acc: 60.77%\n",
            "------------------------------------------------------------\n",
            "Epoch 96/250 - Train Loss: 0.021882, Train Acc: 99.56%\n",
            "            - Test Loss: 2.165477, Test Acc: 60.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 97/250 - Train Loss: 0.025422, Train Acc: 99.42%\n",
            "            - Test Loss: 2.250473, Test Acc: 59.86%\n",
            "------------------------------------------------------------\n",
            "Epoch 98/250 - Train Loss: 0.029751, Train Acc: 99.31%\n",
            "            - Test Loss: 2.225037, Test Acc: 59.49%\n",
            "------------------------------------------------------------\n",
            "Epoch 99/250 - Train Loss: 0.020861, Train Acc: 99.58%\n",
            "            - Test Loss: 2.219084, Test Acc: 59.73%\n",
            "------------------------------------------------------------\n",
            "Epoch 100/250 - Train Loss: 0.032241, Train Acc: 99.31%\n",
            "            - Test Loss: 2.162864, Test Acc: 59.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 101/250 - Train Loss: 0.027609, Train Acc: 99.30%\n",
            "            - Test Loss: 2.301487, Test Acc: 60.68%\n",
            "------------------------------------------------------------\n",
            "Epoch 102/250 - Train Loss: 0.025246, Train Acc: 99.51%\n",
            "            - Test Loss: 2.158615, Test Acc: 59.76%\n",
            "------------------------------------------------------------\n",
            "Epoch 103/250 - Train Loss: 0.018053, Train Acc: 99.65%\n",
            "            - Test Loss: 2.265437, Test Acc: 60.18%\n",
            "------------------------------------------------------------\n",
            "Epoch 104/250 - Train Loss: 0.102275, Train Acc: 96.96%\n",
            "            - Test Loss: 2.410278, Test Acc: 57.95%\n",
            "------------------------------------------------------------\n",
            "Epoch 105/250 - Train Loss: 0.061639, Train Acc: 98.15%\n",
            "            - Test Loss: 2.556999, Test Acc: 58.94%\n",
            "------------------------------------------------------------\n",
            "Epoch 106/250 - Train Loss: 0.079830, Train Acc: 97.65%\n",
            "            - Test Loss: 2.423108, Test Acc: 57.74%\n",
            "------------------------------------------------------------\n",
            "Epoch 107/250 - Train Loss: 0.019645, Train Acc: 99.60%\n",
            "            - Test Loss: 2.220900, Test Acc: 60.31%\n",
            "------------------------------------------------------------\n",
            "Epoch 108/250 - Train Loss: 0.032484, Train Acc: 99.24%\n",
            "            - Test Loss: 2.227885, Test Acc: 59.59%\n",
            "------------------------------------------------------------\n",
            "Epoch 109/250 - Train Loss: 0.020207, Train Acc: 99.55%\n",
            "            - Test Loss: 2.233353, Test Acc: 60.74%\n",
            "------------------------------------------------------------\n",
            "Epoch 110/250 - Train Loss: 0.016153, Train Acc: 99.74%\n",
            "            - Test Loss: 2.151169, Test Acc: 60.73%\n",
            "------------------------------------------------------------\n",
            "Epoch 111/250 - Train Loss: 0.023912, Train Acc: 99.48%\n",
            "            - Test Loss: 2.342902, Test Acc: 59.22%\n",
            "------------------------------------------------------------\n",
            "Epoch 112/250 - Train Loss: 0.027425, Train Acc: 99.36%\n",
            "            - Test Loss: 2.313946, Test Acc: 59.47%\n",
            "------------------------------------------------------------\n",
            "Epoch 113/250 - Train Loss: 0.015065, Train Acc: 99.71%\n",
            "            - Test Loss: 2.229810, Test Acc: 60.59%\n",
            "------------------------------------------------------------\n",
            "Epoch 114/250 - Train Loss: 0.018161, Train Acc: 99.59%\n",
            "            - Test Loss: 2.246751, Test Acc: 60.63%\n",
            "------------------------------------------------------------\n",
            "Epoch 115/250 - Train Loss: 0.018507, Train Acc: 99.61%\n",
            "            - Test Loss: 2.268030, Test Acc: 60.26%\n",
            "------------------------------------------------------------\n",
            "Epoch 116/250 - Train Loss: 0.034139, Train Acc: 99.20%\n",
            "            - Test Loss: 2.267443, Test Acc: 58.66%\n",
            "------------------------------------------------------------\n",
            "Epoch 117/250 - Train Loss: 0.031097, Train Acc: 99.26%\n",
            "            - Test Loss: 2.349804, Test Acc: 58.96%\n",
            "------------------------------------------------------------\n",
            "Epoch 118/250 - Train Loss: 0.024783, Train Acc: 99.44%\n",
            "            - Test Loss: 2.376260, Test Acc: 59.67%\n",
            "------------------------------------------------------------\n",
            "Epoch 119/250 - Train Loss: 0.015397, Train Acc: 99.69%\n",
            "            - Test Loss: 2.272162, Test Acc: 60.32%\n",
            "------------------------------------------------------------\n",
            "Epoch 120/250 - Train Loss: 0.018319, Train Acc: 99.62%\n",
            "            - Test Loss: 2.358014, Test Acc: 60.05%\n",
            "------------------------------------------------------------\n",
            "Epoch 121/250 - Train Loss: 0.025678, Train Acc: 99.39%\n",
            "            - Test Loss: 2.344421, Test Acc: 59.45%\n",
            "------------------------------------------------------------\n",
            "Epoch 122/250 - Train Loss: 0.023243, Train Acc: 99.49%\n",
            "            - Test Loss: 2.340556, Test Acc: 59.51%\n",
            "------------------------------------------------------------\n",
            "Epoch 123/250 - Train Loss: 0.014514, Train Acc: 99.74%\n",
            "            - Test Loss: 2.392770, Test Acc: 60.00%\n",
            "------------------------------------------------------------\n",
            "Epoch 124/250 - Train Loss: 0.014951, Train Acc: 99.70%\n",
            "            - Test Loss: 2.320417, Test Acc: 60.39%\n",
            "------------------------------------------------------------\n",
            "Epoch 125/250 - Train Loss: 0.019931, Train Acc: 99.50%\n",
            "            - Test Loss: 2.381622, Test Acc: 60.14%\n",
            "------------------------------------------------------------\n",
            "Epoch 126/250 - Train Loss: 0.014733, Train Acc: 99.71%\n",
            "            - Test Loss: 2.324003, Test Acc: 60.44%\n",
            "------------------------------------------------------------\n",
            "Epoch 127/250 - Train Loss: 0.018877, Train Acc: 99.63%\n",
            "            - Test Loss: 2.297261, Test Acc: 60.02%\n",
            "------------------------------------------------------------\n",
            "Epoch 128/250 - Train Loss: 0.022546, Train Acc: 99.48%\n",
            "            - Test Loss: 2.426041, Test Acc: 58.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 129/250 - Train Loss: 0.022899, Train Acc: 99.51%\n",
            "            - Test Loss: 2.320181, Test Acc: 59.06%\n",
            "------------------------------------------------------------\n",
            "Epoch 130/250 - Train Loss: 0.015211, Train Acc: 99.71%\n",
            "            - Test Loss: 2.343998, Test Acc: 59.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 131/250 - Train Loss: 0.032716, Train Acc: 98.98%\n",
            "            - Test Loss: 2.756161, Test Acc: 59.36%\n",
            "------------------------------------------------------------\n",
            "Epoch 132/250 - Train Loss: 0.014240, Train Acc: 99.65%\n",
            "            - Test Loss: 2.428484, Test Acc: 60.24%\n",
            "------------------------------------------------------------\n",
            "Epoch 133/250 - Train Loss: 0.021638, Train Acc: 99.38%\n",
            "            - Test Loss: 2.552313, Test Acc: 60.48%\n",
            "------------------------------------------------------------\n",
            "Epoch 134/250 - Train Loss: 0.012060, Train Acc: 99.73%\n",
            "            - Test Loss: 2.453956, Test Acc: 60.44%\n",
            "------------------------------------------------------------\n",
            "Epoch 135/250 - Train Loss: 0.023802, Train Acc: 99.44%\n",
            "            - Test Loss: 2.454615, Test Acc: 59.33%\n",
            "------------------------------------------------------------\n",
            "Epoch 136/250 - Train Loss: 0.025368, Train Acc: 99.39%\n",
            "            - Test Loss: 2.410997, Test Acc: 60.08%\n",
            "------------------------------------------------------------\n",
            "Epoch 137/250 - Train Loss: 0.012883, Train Acc: 99.75%\n",
            "            - Test Loss: 2.439882, Test Acc: 60.23%\n",
            "------------------------------------------------------------\n",
            "Epoch 138/250 - Train Loss: 0.014841, Train Acc: 99.68%\n",
            "            - Test Loss: 2.446797, Test Acc: 60.43%\n",
            "------------------------------------------------------------\n",
            "Epoch 139/250 - Train Loss: 0.009755, Train Acc: 99.81%\n",
            "            - Test Loss: 2.422689, Test Acc: 60.45%\n",
            "------------------------------------------------------------\n",
            "Epoch 140/250 - Train Loss: 0.014938, Train Acc: 99.67%\n",
            "            - Test Loss: 2.402548, Test Acc: 59.71%\n",
            "------------------------------------------------------------\n",
            "Epoch 141/250 - Train Loss: 0.014547, Train Acc: 99.65%\n",
            "            - Test Loss: 2.496939, Test Acc: 60.09%\n",
            "------------------------------------------------------------\n",
            "Epoch 142/250 - Train Loss: 0.019484, Train Acc: 99.59%\n",
            "            - Test Loss: 2.369634, Test Acc: 60.12%\n",
            "------------------------------------------------------------\n",
            "Epoch 143/250 - Train Loss: 0.012702, Train Acc: 99.73%\n",
            "            - Test Loss: 2.550212, Test Acc: 59.57%\n",
            "------------------------------------------------------------\n",
            "Epoch 144/250 - Train Loss: 0.014453, Train Acc: 99.70%\n",
            "            - Test Loss: 2.501317, Test Acc: 59.69%\n",
            "------------------------------------------------------------\n",
            "Epoch 145/250 - Train Loss: 0.010701, Train Acc: 99.77%\n",
            "            - Test Loss: 2.429688, Test Acc: 60.27%\n",
            "------------------------------------------------------------\n",
            "Epoch 146/250 - Train Loss: 0.009557, Train Acc: 99.81%\n",
            "            - Test Loss: 2.375744, Test Acc: 60.54%\n",
            "------------------------------------------------------------\n",
            "Epoch 147/250 - Train Loss: 0.069057, Train Acc: 98.27%\n",
            "            - Test Loss: 2.525948, Test Acc: 56.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 148/250 - Train Loss: 0.009895, Train Acc: 99.79%\n",
            "            - Test Loss: 2.416375, Test Acc: 60.77%\n",
            "------------------------------------------------------------\n",
            "Epoch 149/250 - Train Loss: 0.014584, Train Acc: 99.61%\n",
            "            - Test Loss: 2.634800, Test Acc: 59.69%\n",
            "------------------------------------------------------------\n",
            "Epoch 150/250 - Train Loss: 0.008544, Train Acc: 99.81%\n",
            "            - Test Loss: 2.529279, Test Acc: 60.43%\n",
            "------------------------------------------------------------\n",
            "Epoch 151/250 - Train Loss: 0.017977, Train Acc: 99.45%\n",
            "            - Test Loss: 2.770444, Test Acc: 60.01%\n",
            "------------------------------------------------------------\n",
            "Epoch 152/250 - Train Loss: 0.008881, Train Acc: 99.81%\n",
            "            - Test Loss: 2.474927, Test Acc: 61.07%\n",
            "------------------------------------------------------------\n",
            "Epoch 153/250 - Train Loss: 0.010891, Train Acc: 99.74%\n",
            "            - Test Loss: 2.564721, Test Acc: 60.37%\n",
            "------------------------------------------------------------\n",
            "Epoch 154/250 - Train Loss: 0.007874, Train Acc: 99.85%\n",
            "            - Test Loss: 2.452623, Test Acc: 60.58%\n",
            "------------------------------------------------------------\n",
            "Epoch 155/250 - Train Loss: 0.010575, Train Acc: 99.79%\n",
            "            - Test Loss: 2.487018, Test Acc: 60.05%\n",
            "------------------------------------------------------------\n",
            "Epoch 156/250 - Train Loss: 0.011457, Train Acc: 99.75%\n",
            "            - Test Loss: 2.518885, Test Acc: 60.32%\n",
            "------------------------------------------------------------\n",
            "Epoch 157/250 - Train Loss: 0.009234, Train Acc: 99.80%\n",
            "            - Test Loss: 2.559727, Test Acc: 60.41%\n",
            "------------------------------------------------------------\n",
            "Epoch 158/250 - Train Loss: 0.013859, Train Acc: 99.71%\n",
            "            - Test Loss: 2.513172, Test Acc: 60.22%\n",
            "------------------------------------------------------------\n",
            "Epoch 159/250 - Train Loss: 0.008324, Train Acc: 99.84%\n",
            "            - Test Loss: 2.561714, Test Acc: 60.20%\n",
            "------------------------------------------------------------\n",
            "Epoch 160/250 - Train Loss: 0.016453, Train Acc: 99.65%\n",
            "            - Test Loss: 2.472652, Test Acc: 60.70%\n",
            "------------------------------------------------------------\n",
            "Epoch 161/250 - Train Loss: 0.015545, Train Acc: 99.78%\n",
            "            - Test Loss: 2.488000, Test Acc: 61.12%\n",
            "------------------------------------------------------------\n",
            "Epoch 162/250 - Train Loss: 0.018377, Train Acc: 99.72%\n",
            "            - Test Loss: 2.441127, Test Acc: 60.32%\n",
            "------------------------------------------------------------\n",
            "Epoch 163/250 - Train Loss: 0.010298, Train Acc: 99.78%\n",
            "            - Test Loss: 2.644205, Test Acc: 61.13%\n",
            "------------------------------------------------------------\n",
            "Epoch 164/250 - Train Loss: 0.013427, Train Acc: 99.75%\n",
            "            - Test Loss: 2.620748, Test Acc: 60.02%\n",
            "------------------------------------------------------------\n",
            "Epoch 165/250 - Train Loss: 0.010455, Train Acc: 99.78%\n",
            "            - Test Loss: 2.440335, Test Acc: 60.07%\n",
            "------------------------------------------------------------\n",
            "Epoch 166/250 - Train Loss: 0.012903, Train Acc: 99.68%\n",
            "            - Test Loss: 2.532941, Test Acc: 60.03%\n",
            "------------------------------------------------------------\n",
            "Epoch 167/250 - Train Loss: 0.036062, Train Acc: 99.13%\n",
            "            - Test Loss: 2.521170, Test Acc: 58.26%\n",
            "------------------------------------------------------------\n",
            "Epoch 168/250 - Train Loss: 0.008016, Train Acc: 99.84%\n",
            "            - Test Loss: 2.520871, Test Acc: 60.68%\n",
            "------------------------------------------------------------\n",
            "Epoch 169/250 - Train Loss: 0.027184, Train Acc: 99.52%\n",
            "            - Test Loss: 2.432666, Test Acc: 58.74%\n",
            "------------------------------------------------------------\n",
            "Epoch 170/250 - Train Loss: 0.030338, Train Acc: 99.31%\n",
            "            - Test Loss: 2.419720, Test Acc: 58.56%\n",
            "------------------------------------------------------------\n",
            "Epoch 171/250 - Train Loss: 0.013572, Train Acc: 99.69%\n",
            "            - Test Loss: 2.678954, Test Acc: 59.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 172/250 - Train Loss: 0.009006, Train Acc: 99.82%\n",
            "            - Test Loss: 2.585968, Test Acc: 60.96%\n",
            "------------------------------------------------------------\n",
            "Epoch 173/250 - Train Loss: 0.010307, Train Acc: 99.79%\n",
            "            - Test Loss: 2.493276, Test Acc: 60.79%\n",
            "------------------------------------------------------------\n",
            "Epoch 174/250 - Train Loss: 0.012481, Train Acc: 99.72%\n",
            "            - Test Loss: 2.552555, Test Acc: 60.51%\n",
            "------------------------------------------------------------\n",
            "Epoch 175/250 - Train Loss: 0.008010, Train Acc: 99.84%\n",
            "            - Test Loss: 2.531457, Test Acc: 61.15%\n",
            "------------------------------------------------------------\n",
            "Epoch 176/250 - Train Loss: 0.010096, Train Acc: 99.83%\n",
            "            - Test Loss: 2.588814, Test Acc: 61.10%\n",
            "------------------------------------------------------------\n",
            "Epoch 177/250 - Train Loss: 0.011639, Train Acc: 99.72%\n",
            "            - Test Loss: 2.661686, Test Acc: 60.67%\n",
            "------------------------------------------------------------\n",
            "Epoch 178/250 - Train Loss: 0.008593, Train Acc: 99.83%\n",
            "            - Test Loss: 2.460224, Test Acc: 61.02%\n",
            "------------------------------------------------------------\n",
            "Epoch 179/250 - Train Loss: 0.010387, Train Acc: 99.81%\n",
            "            - Test Loss: 2.519364, Test Acc: 60.26%\n",
            "------------------------------------------------------------\n",
            "Epoch 180/250 - Train Loss: 0.012824, Train Acc: 99.75%\n",
            "            - Test Loss: 2.554387, Test Acc: 61.10%\n",
            "------------------------------------------------------------\n",
            "Epoch 181/250 - Train Loss: 0.007764, Train Acc: 99.82%\n",
            "            - Test Loss: 2.523583, Test Acc: 61.48%\n",
            "------------------------------------------------------------\n",
            "Epoch 182/250 - Train Loss: 0.008559, Train Acc: 99.80%\n",
            "            - Test Loss: 2.704493, Test Acc: 60.95%\n",
            "------------------------------------------------------------\n",
            "Epoch 183/250 - Train Loss: 0.007528, Train Acc: 99.85%\n",
            "            - Test Loss: 2.511630, Test Acc: 61.18%\n",
            "------------------------------------------------------------\n",
            "Epoch 184/250 - Train Loss: 0.011487, Train Acc: 99.78%\n",
            "            - Test Loss: 2.561863, Test Acc: 60.32%\n",
            "------------------------------------------------------------\n",
            "Epoch 185/250 - Train Loss: 0.008317, Train Acc: 99.84%\n",
            "            - Test Loss: 2.686530, Test Acc: 60.74%\n",
            "------------------------------------------------------------\n",
            "Epoch 186/250 - Train Loss: 0.006517, Train Acc: 99.88%\n",
            "            - Test Loss: 2.487800, Test Acc: 60.89%\n",
            "------------------------------------------------------------\n",
            "Epoch 187/250 - Train Loss: 0.019371, Train Acc: 99.49%\n",
            "            - Test Loss: 2.894277, Test Acc: 59.62%\n",
            "------------------------------------------------------------\n",
            "Epoch 188/250 - Train Loss: 0.009450, Train Acc: 99.82%\n",
            "            - Test Loss: 2.573018, Test Acc: 60.72%\n",
            "------------------------------------------------------------\n",
            "Epoch 189/250 - Train Loss: 0.008574, Train Acc: 99.85%\n",
            "            - Test Loss: 2.579199, Test Acc: 60.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 190/250 - Train Loss: 0.012669, Train Acc: 99.82%\n",
            "            - Test Loss: 2.611890, Test Acc: 60.74%\n",
            "------------------------------------------------------------\n",
            "Epoch 191/250 - Train Loss: 0.007844, Train Acc: 99.84%\n",
            "            - Test Loss: 2.726177, Test Acc: 60.72%\n",
            "------------------------------------------------------------\n",
            "Epoch 192/250 - Train Loss: 0.008801, Train Acc: 99.77%\n",
            "            - Test Loss: 2.805126, Test Acc: 60.75%\n",
            "------------------------------------------------------------\n",
            "Epoch 193/250 - Train Loss: 0.012793, Train Acc: 99.76%\n",
            "            - Test Loss: 2.569218, Test Acc: 60.29%\n",
            "------------------------------------------------------------\n",
            "Epoch 194/250 - Train Loss: 0.007464, Train Acc: 99.85%\n",
            "            - Test Loss: 2.617847, Test Acc: 60.79%\n",
            "------------------------------------------------------------\n",
            "Epoch 195/250 - Train Loss: 0.006536, Train Acc: 99.87%\n",
            "            - Test Loss: 2.674002, Test Acc: 60.91%\n",
            "------------------------------------------------------------\n",
            "Epoch 196/250 - Train Loss: 0.010257, Train Acc: 99.75%\n",
            "            - Test Loss: 2.724964, Test Acc: 60.42%\n",
            "------------------------------------------------------------\n",
            "Epoch 197/250 - Train Loss: 0.010027, Train Acc: 99.78%\n",
            "            - Test Loss: 2.712628, Test Acc: 60.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 198/250 - Train Loss: 0.006608, Train Acc: 99.87%\n",
            "            - Test Loss: 2.518479, Test Acc: 60.63%\n",
            "------------------------------------------------------------\n",
            "Epoch 199/250 - Train Loss: 0.006882, Train Acc: 99.89%\n",
            "            - Test Loss: 2.470812, Test Acc: 60.83%\n",
            "------------------------------------------------------------\n",
            "Epoch 200/250 - Train Loss: 0.034337, Train Acc: 99.77%\n",
            "            - Test Loss: 2.641956, Test Acc: 60.97%\n",
            "------------------------------------------------------------\n",
            "Epoch 201/250 - Train Loss: 0.007738, Train Acc: 99.89%\n",
            "            - Test Loss: 2.648299, Test Acc: 61.05%\n",
            "------------------------------------------------------------\n",
            "Epoch 202/250 - Train Loss: 0.016725, Train Acc: 99.76%\n",
            "            - Test Loss: 2.538308, Test Acc: 60.64%\n",
            "------------------------------------------------------------\n",
            "Epoch 203/250 - Train Loss: 0.012480, Train Acc: 99.75%\n",
            "            - Test Loss: 2.549637, Test Acc: 60.38%\n",
            "------------------------------------------------------------\n",
            "Epoch 204/250 - Train Loss: 0.007082, Train Acc: 99.86%\n",
            "            - Test Loss: 2.633790, Test Acc: 61.27%\n",
            "------------------------------------------------------------\n",
            "Epoch 205/250 - Train Loss: 0.010233, Train Acc: 99.82%\n",
            "            - Test Loss: 2.463897, Test Acc: 60.14%\n",
            "------------------------------------------------------------\n",
            "Epoch 206/250 - Train Loss: 0.014860, Train Acc: 99.78%\n",
            "            - Test Loss: 2.857298, Test Acc: 60.80%\n",
            "------------------------------------------------------------\n",
            "Epoch 207/250 - Train Loss: 0.016071, Train Acc: 99.86%\n",
            "            - Test Loss: 2.685426, Test Acc: 60.72%\n",
            "------------------------------------------------------------\n",
            "Epoch 208/250 - Train Loss: 0.008184, Train Acc: 99.86%\n",
            "            - Test Loss: 2.779163, Test Acc: 60.64%\n",
            "------------------------------------------------------------\n",
            "Epoch 209/250 - Train Loss: 0.007562, Train Acc: 99.89%\n",
            "            - Test Loss: 2.582390, Test Acc: 61.31%\n",
            "------------------------------------------------------------\n",
            "Epoch 210/250 - Train Loss: 0.007874, Train Acc: 99.89%\n",
            "            - Test Loss: 2.557081, Test Acc: 60.85%\n",
            "------------------------------------------------------------\n",
            "Epoch 211/250 - Train Loss: 0.005501, Train Acc: 99.86%\n",
            "            - Test Loss: 2.675678, Test Acc: 61.29%\n",
            "------------------------------------------------------------\n",
            "Epoch 212/250 - Train Loss: 0.009257, Train Acc: 99.78%\n",
            "            - Test Loss: 2.652434, Test Acc: 60.43%\n",
            "------------------------------------------------------------\n",
            "Epoch 213/250 - Train Loss: 0.010044, Train Acc: 99.87%\n",
            "            - Test Loss: 2.601553, Test Acc: 60.81%\n",
            "------------------------------------------------------------\n",
            "Epoch 214/250 - Train Loss: 0.013025, Train Acc: 99.89%\n",
            "            - Test Loss: 2.568704, Test Acc: 61.20%\n",
            "------------------------------------------------------------\n",
            "Epoch 215/250 - Train Loss: 0.025673, Train Acc: 99.31%\n",
            "            - Test Loss: 2.995550, Test Acc: 58.82%\n",
            "------------------------------------------------------------\n",
            "Epoch 216/250 - Train Loss: 0.009100, Train Acc: 99.84%\n",
            "            - Test Loss: 2.721915, Test Acc: 61.05%\n",
            "------------------------------------------------------------\n",
            "Epoch 217/250 - Train Loss: 0.012146, Train Acc: 99.85%\n",
            "            - Test Loss: 2.646975, Test Acc: 61.04%\n",
            "------------------------------------------------------------\n",
            "Epoch 218/250 - Train Loss: 0.005742, Train Acc: 99.87%\n",
            "            - Test Loss: 2.724494, Test Acc: 61.11%\n",
            "------------------------------------------------------------\n",
            "Epoch 219/250 - Train Loss: 0.023793, Train Acc: 99.79%\n",
            "            - Test Loss: 2.652608, Test Acc: 60.93%\n",
            "------------------------------------------------------------\n",
            "Epoch 220/250 - Train Loss: 0.010028, Train Acc: 99.86%\n",
            "            - Test Loss: 2.710294, Test Acc: 61.25%\n",
            "------------------------------------------------------------\n",
            "Epoch 221/250 - Train Loss: 0.015166, Train Acc: 99.81%\n",
            "            - Test Loss: 2.614326, Test Acc: 60.67%\n",
            "------------------------------------------------------------\n",
            "Epoch 222/250 - Train Loss: 0.012821, Train Acc: 99.87%\n",
            "            - Test Loss: 2.631094, Test Acc: 60.78%\n",
            "------------------------------------------------------------\n",
            "Epoch 223/250 - Train Loss: 0.012874, Train Acc: 99.81%\n",
            "            - Test Loss: 2.604567, Test Acc: 60.62%\n",
            "------------------------------------------------------------\n",
            "Epoch 224/250 - Train Loss: 0.056561, Train Acc: 99.74%\n",
            "            - Test Loss: 2.996315, Test Acc: 60.93%\n",
            "------------------------------------------------------------\n",
            "Epoch 225/250 - Train Loss: 0.005605, Train Acc: 99.91%\n",
            "            - Test Loss: 2.699171, Test Acc: 61.36%\n",
            "------------------------------------------------------------\n",
            "Epoch 226/250 - Train Loss: 0.006819, Train Acc: 99.78%\n",
            "            - Test Loss: 3.017420, Test Acc: 60.88%\n",
            "------------------------------------------------------------\n",
            "Epoch 227/250 - Train Loss: 0.010578, Train Acc: 99.78%\n",
            "            - Test Loss: 2.969318, Test Acc: 60.16%\n",
            "------------------------------------------------------------\n",
            "Epoch 228/250 - Train Loss: 0.014764, Train Acc: 99.87%\n",
            "            - Test Loss: 2.444319, Test Acc: 59.66%\n",
            "------------------------------------------------------------\n",
            "Epoch 229/250 - Train Loss: 0.012477, Train Acc: 99.81%\n",
            "            - Test Loss: 2.824230, Test Acc: 60.71%\n",
            "------------------------------------------------------------\n",
            "Epoch 230/250 - Train Loss: 0.012503, Train Acc: 99.84%\n",
            "            - Test Loss: 2.737956, Test Acc: 60.47%\n",
            "------------------------------------------------------------\n",
            "Epoch 231/250 - Train Loss: 0.010933, Train Acc: 99.84%\n",
            "            - Test Loss: 2.672483, Test Acc: 61.26%\n",
            "------------------------------------------------------------\n",
            "Epoch 232/250 - Train Loss: 0.011672, Train Acc: 99.79%\n",
            "            - Test Loss: 2.860473, Test Acc: 61.16%\n",
            "------------------------------------------------------------\n",
            "Epoch 233/250 - Train Loss: 0.011589, Train Acc: 99.84%\n",
            "            - Test Loss: 2.517003, Test Acc: 60.98%\n",
            "------------------------------------------------------------\n",
            "Epoch 234/250 - Train Loss: 0.006381, Train Acc: 99.88%\n",
            "            - Test Loss: 2.444982, Test Acc: 61.20%\n",
            "------------------------------------------------------------\n",
            "Epoch 235/250 - Train Loss: 0.028192, Train Acc: 99.55%\n",
            "            - Test Loss: 2.775018, Test Acc: 59.52%\n",
            "------------------------------------------------------------\n",
            "Epoch 236/250 - Train Loss: 0.032974, Train Acc: 99.62%\n",
            "            - Test Loss: 2.759924, Test Acc: 60.19%\n",
            "------------------------------------------------------------\n",
            "Epoch 237/250 - Train Loss: 0.012553, Train Acc: 99.87%\n",
            "            - Test Loss: 2.589979, Test Acc: 61.12%\n",
            "------------------------------------------------------------\n",
            "Epoch 238/250 - Train Loss: 0.009439, Train Acc: 99.76%\n",
            "            - Test Loss: 2.955262, Test Acc: 60.64%\n",
            "------------------------------------------------------------\n",
            "Epoch 239/250 - Train Loss: 0.005836, Train Acc: 99.85%\n",
            "            - Test Loss: 2.877357, Test Acc: 61.06%\n",
            "------------------------------------------------------------\n",
            "Epoch 240/250 - Train Loss: 0.008679, Train Acc: 99.87%\n",
            "            - Test Loss: 2.534028, Test Acc: 60.63%\n",
            "------------------------------------------------------------\n",
            "Epoch 241/250 - Train Loss: 0.008402, Train Acc: 99.87%\n",
            "            - Test Loss: 2.788182, Test Acc: 61.44%\n",
            "------------------------------------------------------------\n",
            "Epoch 242/250 - Train Loss: 0.038174, Train Acc: 99.81%\n",
            "            - Test Loss: 2.392487, Test Acc: 60.86%\n",
            "------------------------------------------------------------\n",
            "Epoch 243/250 - Train Loss: 0.037029, Train Acc: 99.73%\n",
            "            - Test Loss: 2.775452, Test Acc: 60.36%\n",
            "------------------------------------------------------------\n",
            "Epoch 244/250 - Train Loss: 0.005788, Train Acc: 99.86%\n",
            "            - Test Loss: 2.812110, Test Acc: 60.16%\n",
            "------------------------------------------------------------\n",
            "Epoch 245/250 - Train Loss: 0.005622, Train Acc: 99.88%\n",
            "            - Test Loss: 2.658845, Test Acc: 61.53%\n",
            "------------------------------------------------------------\n",
            "Epoch 246/250 - Train Loss: 0.008361, Train Acc: 99.86%\n",
            "            - Test Loss: 2.702960, Test Acc: 60.42%\n",
            "------------------------------------------------------------\n",
            "Epoch 247/250 - Train Loss: 0.018123, Train Acc: 99.80%\n",
            "            - Test Loss: 2.728531, Test Acc: 60.06%\n",
            "------------------------------------------------------------\n",
            "Epoch 248/250 - Train Loss: 0.007705, Train Acc: 99.86%\n",
            "            - Test Loss: 2.735320, Test Acc: 60.97%\n",
            "------------------------------------------------------------\n",
            "Epoch 249/250 - Train Loss: 0.004426, Train Acc: 99.90%\n",
            "            - Test Loss: 2.662338, Test Acc: 61.45%\n",
            "------------------------------------------------------------\n",
            "Epoch 250/250 - Train Loss: 0.023574, Train Acc: 99.81%\n",
            "            - Test Loss: 2.570597, Test Acc: 60.90%\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"state\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "test_ids = test_data[\"ID\"]\n",
        "image_data = test_data.drop(columns=[\"ID\"]).values\n",
        "\n",
        "num_samples = image_data.shape[0]\n",
        "images = image_data.reshape(num_samples, 3, 32, 32).astype('float32')\n",
        "\n",
        "images_tensor = torch.tensor(images)"
      ],
      "metadata": {
        "id": "3bFwrBjaT9fx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(images_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "aqMcmAwtUVnr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images_batch in test_loader:\n",
        "        images_batch = images_batch[0].to(device)\n",
        "        outputs = model(images_batch)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        predictions.extend(predicted_labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "7Yyfv6q7Ut4N"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_ids,\n",
        "    \"Label\": predictions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Predictions saved to submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0zhH3W0UyKI",
        "outputId": "6a1f4761-a7f6-43c1-dbb2-48ea6a5cb614"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to submission.csv\n"
          ]
        }
      ]
    }
  ]
}