{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak-hannou/compsci-4ml3/blob/main/F24_4ML3_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42EXGRkhdvSi"
      },
      "source": [
        "# Submit your pdf report as well as your ipynb file to Avenue (separate files). Make sure your pdf report includes all the plots, results, and analyses.\n",
        "\n",
        "\n",
        "# SVM and Logistic Regression with PyTorch Implementation\n",
        "In this exercise, we explore a machine learning framework called PyTorch. We will use the framework to implement SVM and Logistic Regression models and address a multi-class image classification problem.\n",
        "\n",
        "# Submission\n",
        "- There are four tasks for you.\n",
        "- Report the results and answer the questions in a pdf file.\n",
        "- Additionally, submit your code in the same Jupiter notebook format. (keep the overal format of the notebook unchanged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2h_5i8D2Ysd"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for training a neural network on the MNIST dataset\n",
        "\n",
        "# Learning rate controls the step size of gradient descent, determining how much to update the weights in each iteration.\n",
        "learning_rate = 0.02\n",
        "\n",
        "# Number of epochs specifies how many complete passes through the training dataset will be performed.\n",
        "num_epochs = 100\n",
        "\n",
        "# Batch size defines the number of images processed in each mini-batch during training.\n",
        "batch_size = 32\n",
        "\n",
        "# Percentage of the dataset to be used for training, allowing for experimentation with smaller subsets of data.\n",
        "train_data_percentage = 0.0025\n",
        "test_data_percentage = 0.1\n",
        "\n",
        "# Input size corresponds to the dimensions of the MNIST images, flattened from 28x28 pixels.\n",
        "input_size = 28 * 28  # MNIST image size\n",
        "\n",
        "# Number of output classes, representing the digits 0 through 9 in the MNIST dataset.\n",
        "num_classes = 10  # Number of classes in MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf_wVsrPP1Je"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfovmg2QPh-V"
      },
      "source": [
        "# Dataset characteristics\n",
        "We use the MNIST digit classification data set for this assignment.\n",
        "A total of 60K images for training and 10K images for testing are available. But we only use a small percentage of them. Images are 28 x 28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJlI-SUQnGd-"
      },
      "outputs": [],
      "source": [
        "# Load MNIST datasets, and create pytorch data loader to read data in mini-batches\n",
        "def get_data_loaders(train_data_percentage, test_data_percentage, batch_size, transform):\n",
        "    # Load the entire MNIST dataset\n",
        "    # For train and test data points we sometimes use different transforms.\n",
        "    # This becomes handy in the last task (data augmentation)\n",
        "    full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    # Calculate the size based on the percentage\n",
        "    train_size = int(train_data_percentage * len(full_train_dataset))  # percentage of training data\n",
        "    test_size = int(test_data_percentage * len(full_test_dataset))    # percentage of test data\n",
        "    train_remainder = len(full_train_dataset) - train_size\n",
        "    test_remainder = len(full_test_dataset) - test_size\n",
        "\n",
        "    # Split the dataset into the percentage specified and the remaining\n",
        "    train_dataset, _ = random_split(full_train_dataset, [train_size, train_remainder])\n",
        "    test_dataset, _ = random_split(full_test_dataset, [test_size, test_remainder])\n",
        "\n",
        "    # Create DataLoaders for batching and shuffling\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8msoGzhPi4N"
      },
      "outputs": [],
      "source": [
        "# Define transformation (convert to tensor)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_loader, test_loader = get_data_loaders(train_data_percentage, test_data_percentage,batch_size, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J9VN0SrQ-wb"
      },
      "outputs": [],
      "source": [
        "# Get first batch of images and labels\n",
        "train_image_batch, classe_set = next(iter(train_loader))\n",
        "\n",
        "print(f'train_loader contains {len(train_loader)} batches of data.')\n",
        "print(f'train_image_batch has shape {train_image_batch.shape},')\n",
        "print('where 64 is the number of images in a batch, 1 is the number of image channels (1 for grayscale image),\\\n",
        " 28X28 stands for WxH (width and height of a single image).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xI_e2XvQg-u"
      },
      "source": [
        "# Visualization of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOaylU3kQi6I"
      },
      "outputs": [],
      "source": [
        "def show_gray_digits(image_set, row=2, col=3):\n",
        "    # Here we visualize some of the data points in the data set.\n",
        "    # Create a large figure, to be filled with multiple subplots.\n",
        "\n",
        "    # Since image_set is a tensor variable, we transform it to a numpy type variable.\n",
        "    image_set = image_set.cpu().detach().numpy()\n",
        "\n",
        "    for i in range(row*col):\n",
        "      # define subplot\n",
        "      plt.subplot(row, col, i+1)\n",
        "      # plot raw pixel data\n",
        "      plt.imshow(image_set[i,0], cmap=plt.get_cmap('gray'))\n",
        "    # show the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCdEakslSrWJ"
      },
      "outputs": [],
      "source": [
        "# display images and their corresponding labels.\n",
        "show_gray_digits(train_image_batch, 2, 3)\n",
        "print(classe_set[:6])\n",
        "\n",
        "del train_image_batch, classe_set, train_loader, test_loader, transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJiVvIi7DNi"
      },
      "source": [
        "# Task 1: Linear SVM for MNIST classification (30 points).\n",
        "In this part, you are provided with the implementation of a multi-class linear SVM model and the basic MNIST classfication workflow. Run the given code and try to understand how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUTYN2YzXtGJ"
      },
      "source": [
        "## Part A (5 points):\n",
        "\n",
        "Is the implementation of the multi-class linear SVM similar to the end-to-end multiclass SVM that we learned in the class? Are there any significant differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06OPDpdTX87w"
      },
      "source": [
        "## Part B (15 points):\n",
        "1. Compute the accuracy on the train and test set after each epoch in the training. Plot these accuracies as a function of the epoch number and include it in the report (include only the plot in your report, not all the 2*100 numbers).\n",
        "\n",
        "2. Compute the hinge loss on the train and test set after each epoch in the training. Plot these loss values as a function of the epoch number and include it in the report.(include only the plot in your report, not all the 2*100 numbers)\n",
        "\n",
        "3. Report the **last** epoch results (including loss values and accuracies) for both train and test sets.\n",
        "\n",
        "4. Does the model shows significant overfitting? Or do you think there might be other factors that are more significant in the mediocre performance of the model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V106p68mxF2f"
      },
      "outputs": [],
      "source": [
        "# Function to plot train/test loss and accuracy on separate subplots\n",
        "def plot_eval_results(train_loss, test_loss, train_acc, test_acc):\n",
        "    \"\"\"\n",
        "    Plots the training and testing loss/accuracy over the number of epochs.\n",
        "\n",
        "    Parameters:\n",
        "    - train_loss: list or array, the training loss values over epochs.\n",
        "    - test_loss: list or array, the testing loss values over epochs.\n",
        "    - train_acc: list or array, the training accuracy values over epochs.\n",
        "    - test_acc: list or array, the testing accuracy values over epochs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Create subplots (1 row, 2 columns) for loss and accuracy\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # figsize sets the overall plot size\n",
        "\n",
        "    # Plot Loss on the first subplot\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    # Plot Accuracy on the second subplot\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    # Adjust layout to avoid subplot overlap\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the plots\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoFONXckubrE"
      },
      "outputs": [],
      "source": [
        "# Define a linear SVM model\n",
        "class LinearSVM(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LinearSVM, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the image\n",
        "        x= x.view(-1, input_size)\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_X_hxJyuiyN"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_loader, test_loader = get_data_loaders(train_data_percentage, test_data_percentage, batch_size, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UlGqcoeoRJX"
      },
      "outputs": [],
      "source": [
        "# Return model loss and accuracy with the provided criterion and data_loader.\n",
        "def eval(model, data_loader, criterion=None):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  loss_batches = []\n",
        "\n",
        "  # Switch to evaluation mode and turn off gradient calculation\n",
        "  # since parameters are not updated during testing.\n",
        "  with torch.no_grad():\n",
        "      for images_batch, labels_batch in data_loader:\n",
        "          outputs = model(images_batch)\n",
        "          # The predicted label is the output with the highest activation.\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels_batch.size(0)\n",
        "          correct += (predicted == labels_batch).sum().item()\n",
        "\n",
        "          # Use provided criterion to calculate the loss for the mini batch\n",
        "          # Append the mini-batch loss to loss_batches array\n",
        "          batch_loss = criterion(outputs, labels_batch)\n",
        "          loss_batches.append(batch_loss.item())\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  loss = np.mean(loss_batches)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return accuracy, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5F9pY_zdpNs"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = LinearSVM(input_size, num_classes)\n",
        "criterion = nn.MultiMarginLoss()  # A Multi-class version of Hinge loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_loss_epochs = []\n",
        "test_loss_epochs = []\n",
        "train_accuracy_epochs = []\n",
        "test_accuracy_epochs = []\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images_batch, labels_batch) in enumerate(train_loader):\n",
        "        optimizer.zero_grad() # Clear the gradients\n",
        "        outputs = model(images_batch) # Forward pass\n",
        "        loss = criterion(outputs, labels_batch) # Calculate loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "    # Obtain train/test loss values and accuracies after each epoch\n",
        "    train_accuracy, train_loss = eval(model, train_loader, criterion)\n",
        "    test_accuracy, test_loss = eval(model, test_loader, criterion)\n",
        "\n",
        "    print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_accuracy:.2f}%')\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "\n",
        "    train_loss_epochs.append(train_loss)\n",
        "    test_loss_epochs.append(test_loss)\n",
        "    train_accuracy_epochs.append(train_accuracy)\n",
        "    test_accuracy_epochs.append(test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxwxf94IvSLJ"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4mZ3HLrmN1X"
      },
      "source": [
        "## Part C (10 points):\n",
        "\n",
        "Weight decay works like regularization. Set weight decay to each of the values (0.1, 1, 10) during defining the SGD optimizer (see  [SGD optimizer documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) for how to do that). Plot the train/test losses and accuracies per epoch. Also report the last epoch results (loss and accuracy for both train and test) . Does weight decay help in this case?Justify the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFRE4-dKmLkm"
      },
      "outputs": [],
      "source": [
        "for weight_decay in [0.1, 1, 10]:\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    # Plot accuracies\n",
        "    print(f'For weight decay value {weight_decay}:')\n",
        "    plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWxhX7sa7Sas"
      },
      "source": [
        "# Task 2: Logistic Regression for MNIST classification  (20 points).\n",
        "In this part, you are asked to implement a logistic regression model for the same MNIST classfication problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eLV47R7YFgK"
      },
      "source": [
        "## Part A (5 points):\n",
        "Use Cross Entropy Loss (rather than Hinge Loss) to implement logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrHPqeNjYHbY"
      },
      "source": [
        "\n",
        "## Part B (10 points):\n",
        "1. Compute the accuracy on the train and test set after each epoch in the training. Plot these accuracies as a function of the epoch number.\n",
        "\n",
        "2. Compute the cross-entropy loss on the train and test set after each epoch in the training. Plot these loss values as a function of the epoch number.\n",
        "\n",
        "3. Report the last epoch results (including loss values and accuracies) for both train and test sets.\n",
        "\n",
        "4. Deos the model shows significant overfitting? Or do you think there might be other factors that are more significant in the mediocre performance of the model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9zJ6UzNYI66"
      },
      "source": [
        "## Part C (5 points)\n",
        "\n",
        "Compare the results with SVM model. Does it work better, worse, or similar?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb4NBvIIzml6"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "\n",
        "        ########################\n",
        "        ########################\n",
        "        #### YOUR CODE HERE ####\n",
        "        ########################\n",
        "        ########################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ########################\n",
        "        ########################\n",
        "        #### YOUR CODE HERE ####\n",
        "        ########################\n",
        "        ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRaRLWiz7W9m"
      },
      "outputs": [],
      "source": [
        "# Model initialization\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "# Train logistic regression model for MNIST classification\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_accuracy:.2f}%')\n",
        "    print(\"-------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFY-6zhb0Uet"
      },
      "outputs": [],
      "source": [
        "# Plot the loss values and accuracies for train/test\n",
        "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW6Hed9_7cTl"
      },
      "source": [
        "# Task 3: Non-linearity (30 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxn4V824YZ9U"
      },
      "source": [
        "## Part A (20 points):\n",
        "\n",
        "Add a hidden layer with 5000 neurons and a RELU layer for both logistic regression and SVM models in Task 1 and Task 2.\n",
        "1. For both models, plot the train loss and the test loss.\n",
        "2. For both models, plot the train and test accuracies.\n",
        "3. For both models, report the loss and accuracy for both train and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZbsCwxUYbIY"
      },
      "source": [
        "## Part B (10 Points):\n",
        "\n",
        "Compare the results with the linear model (without weight decay, to keep the comparison fair). Which approach works better? Why? Which appproach is more prone to overfitting? Explain your findings and justify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeaGbUHs7hM0"
      },
      "outputs": [],
      "source": [
        "# Both the SVM and Logistic Regression models we have in Task 1 and 2\n",
        "# can be changed to the ModifiedModel below.\n",
        "\n",
        "# Modified model with added neurons and relu layer\n",
        "class ModifiedModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "\n",
        "        ########################\n",
        "        ########################\n",
        "        #### YOUR CODE HERE ####\n",
        "        ########################\n",
        "        ########################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ########################\n",
        "        ########################\n",
        "        #### YOUR CODE HERE ####\n",
        "        ########################\n",
        "        ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAACCDvKjifY"
      },
      "outputs": [],
      "source": [
        "# Run the ModifiedModel with Hinge Loss (SVM)\n",
        "# Model initialization\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "# Training\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_accuracy:.2f}%')\n",
        "    print(\"-------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss values and accuracies for train/test\n",
        "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
      ],
      "metadata": {
        "id": "04rnHg1kjCsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yU8_zZojZGG"
      },
      "outputs": [],
      "source": [
        "# Run the ModifiedModel with Cross Entropy Loss (Logistic Regression)\n",
        "# Model initialization\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "# Training\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_accuracy:.2f}%')\n",
        "    print(\"-------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss values and accuracies for train/test\n",
        "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
      ],
      "metadata": {
        "id": "1E6gU-VbjHcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q09fVj0IB7LZ"
      },
      "source": [
        "# Task 4: Data Augmentation (20 points)\n",
        "\n",
        "In this task, we will explore the concept of data augmentation, which is a powerful technique used to enhance the diversity of our training dataset without collecting new data. By applying various transformations to the original training images, we can create modified versions of these images.\n",
        "We can then use these modified images to train our model with a \"richer\" set of examples.The use of data augmentation helps to improve the robustness and generalization of our models. Data augmentation is particularly beneficial in tasks like image classification, where we expect the model to be invariant to slight variations of images (e.g., rotation, cropping, blurring, etc.)\n",
        "\n",
        "For this task, you are given a code that uses Gaussian Blur augmentation, which applies a Gaussian filter to slightly blur the images. If you run the code, you will see that this type of augmentation actually makes the model less accurate (compared with Task 3, SVM test accuracy)\n",
        "\n",
        "For this task, you must explore other types of data augmentation and find one that improves the test accuracy by at least 1 percent compared with not using any augmentation (i.e., compared with Task 3, SVM test accuracy). Only change the augmentation approach, and keep the other parts of the code unchanged. Read the PyTorch documentation on different augmentation techniques [here](https://pytorch.org/vision/stable/transforms.html), and then try to identify a good augmentation method from them. Report the augmentation approach that you used, and explain why you think it helps. Also include train/test accuracy plots per epoch, and the train/test accuracy at the final epoch.\n",
        "\n",
        "\n",
        "Even if you find that your method does not improve upon the previous task, submit whatever you have completed to get partial marks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpyJUjzCqguo"
      },
      "outputs": [],
      "source": [
        "# TRANSFOMATION\n",
        "transform = transforms.Compose([\n",
        "    transforms.GaussianBlur(kernel_size=3),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_loader_2, test_loader_2 = get_data_loaders(train_data_percentage, test_data_percentage, batch_size, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpZsz9sjMTfv"
      },
      "outputs": [],
      "source": [
        "# Run the ModifiedModel with Hinge Loss (SVM)\n",
        "# Model initialization\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "# Training\n",
        "\n",
        "    ########################\n",
        "    ########################\n",
        "    #### YOUR CODE HERE ####\n",
        "    ########################\n",
        "    ########################\n",
        "\n",
        "    print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_accuracy:.2f}%')\n",
        "    print(\"-------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss values and accuracies for train/test\n",
        "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
      ],
      "metadata": {
        "id": "uNOYq6y8jLVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}